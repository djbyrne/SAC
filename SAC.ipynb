{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from tensorboardX import SummaryWriter\n",
    "import roboschool\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.linear3 = nn.Linear(hidden_size[1], 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], \n",
    "                 init_w=3e-3, log_std_min=-20, log_std_max=2, epsilon=1e-6):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        \n",
    "        self.mean_linear = nn.Linear(hidden_size[1], num_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "        self.log_std_linear = nn.Linear(hidden_size[1], num_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def rsample(self, return_pretanh_value=False):\n",
    "        \"\"\"\n",
    "        Sampling in the reparameterization case.\n",
    "        \"\"\"\n",
    "        z = (\n",
    "            self.normal_mean +\n",
    "            self.normal_std *\n",
    "            Normal(\n",
    "                ptu.zeros(self.normal_mean.size()),\n",
    "                ptu.ones(self.normal_std.size())\n",
    "            ).sample()\n",
    "        )\n",
    "        z.requires_grad_()\n",
    "\n",
    "        if return_pretanh_value:\n",
    "            return torch.tanh(z), z\n",
    "        else:\n",
    "            return torch.tanh(z)\n",
    "    \n",
    "    def forward(self, state, deterministic=False):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        mean    = self.mean_linear(x)\n",
    "        \n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        log_prob = None\n",
    "        \n",
    "        if deterministic:\n",
    "            action = torch.tanh(mean)\n",
    "        else:\n",
    "            # assumes actions have been normalized to (0,1)\n",
    "            normal = Normal(0, 1)\n",
    "            z = mean + std * normal.sample().requires_grad_()\n",
    "            action = torch.tanh(z)\n",
    "            log_prob = Normal(mean, std).log_prob(z) - torch.log(1 - action * action + self.epsilon)\n",
    "            \n",
    "        return action, mean, log_std, log_prob, std\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action,_,_,_,_ =  self.forward(state, deterministic)\n",
    "        act = action.cpu()[0][0]\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.linear3 = nn.Linear(hidden_size[1], 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        \n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "def normalize_action(action, low, high):\n",
    "    action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "    action = np.clip(action, low, high)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(object):\n",
    "    \n",
    "    def __init__(self, env, replay_buffer, seed=0, hidden_dim=[400,300],\n",
    "        steps_per_epoch=200, epochs=1000, discount=0.99,\n",
    "        tau=1e-2, lr=1e-3, auto_alpha=True, batch_size=100, start_steps=10000,\n",
    "        max_ep_len=200, logger_kwargs=dict(), save_freq=1):\n",
    "        \n",
    "        # Set seeds\n",
    "        self.env = env\n",
    "        self.env.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # env space\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0] \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # init networks\n",
    "        \n",
    "        # Soft Q\n",
    "        self.soft_q_net1 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        self.soft_q_net2 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        self.target_soft_q_net1 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        self.target_soft_q_net2 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "        # Policy\n",
    "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        # Optimizers/Loss\n",
    "        self.soft_q_criterion = nn.MSELoss()\n",
    "        \n",
    "        self.soft_q_optimizer1 = optim.Adam(self.soft_q_net1.parameters(), lr=lr)\n",
    "        self.soft_q_optimizer2 = optim.Adam(self.soft_q_net2.parameters(), lr=lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        \n",
    "        # alpha tuning\n",
    "        self.auto_alpha = auto_alpha\n",
    "        \n",
    "        if self.auto_alpha:\n",
    "            self.target_entropy = -np.prod(env.action_space.shape).item()\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n",
    "            \n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.discount = discount\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        \n",
    "    def get_action(self, state, deterministic=False, explore=False):\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        if explore:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            action  = self.policy_net.get_action(state, deterministic).detach()\n",
    "            return action.numpy()\n",
    "           \n",
    "    def update(self, iterations, batch_size = 100):\n",
    "        \n",
    "        for _ in range(0,iterations):\n",
    "        \n",
    "            state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "            state      = torch.FloatTensor(state).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            action     = torch.FloatTensor(action).to(device)\n",
    "            reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "            done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "            new_actions, policy_mean, policy_log_std, log_pi, *_ = self.policy_net(state)\n",
    "\n",
    "            if self.auto_alpha:\n",
    "                alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "                self.alpha_optimizer.zero_grad()\n",
    "                alpha_loss.backward()\n",
    "                self.alpha_optimizer.step()\n",
    "                alpha = self.log_alpha.exp()\n",
    "            else:\n",
    "                alpha_loss = 0\n",
    "                alpha = 0.2 # constant used by OpenAI\n",
    "\n",
    "            # Update Policy \n",
    "            q_new_actions = torch.min(\n",
    "                self.soft_q_net1(state, new_actions), \n",
    "                self.soft_q_net2(state, new_actions)\n",
    "            )\n",
    "\n",
    "            policy_loss = (alpha*log_pi - q_new_actions).mean()\n",
    "\n",
    "            # Update Soft Q Function\n",
    "            q1_pred = self.soft_q_net1(state, action)\n",
    "            q2_pred = self.soft_q_net2(state, action)\n",
    "\n",
    "            new_next_actions, _, _, new_log_pi, *_ = self.policy_net(next_state)\n",
    "\n",
    "            target_q_values = torch.min(\n",
    "                self.target_soft_q_net1(next_state, new_next_actions),\n",
    "                self.target_soft_q_net2(next_state, new_next_actions),\n",
    "            ) - alpha * new_log_pi\n",
    "\n",
    "            q_target = reward + (1 - done) * self.discount * target_q_values\n",
    "            q1_loss = self.soft_q_criterion(q1_pred, q_target.detach())\n",
    "            q2_loss = self.soft_q_criterion(q2_pred, q_target.detach())\n",
    "\n",
    "            # Update Networks\n",
    "            self.soft_q_optimizer1.zero_grad()\n",
    "            q1_loss.backward()\n",
    "            self.soft_q_optimizer1.step()\n",
    "\n",
    "            self.soft_q_optimizer2.zero_grad()\n",
    "            q2_loss.backward()\n",
    "            self.soft_q_optimizer2.step()\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "            # Soft Updates\n",
    "            for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "                )\n",
    "\n",
    "            for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "                )\n",
    "                \n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.soft_q_net1.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "\n",
    "    def load(self, filename=\"best_avg\", directory=\"./saves\"):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, steps_per_epoch=1000, epochs=100, start_steps=1000, max_ep_len=200):\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"-SAC-Pendulum\")\n",
    "    \n",
    "    # start tracking time\n",
    "    start_time = time.time()\n",
    "    total_rewards = []\n",
    "    avg_reward = None\n",
    "    \n",
    "    # set initial values\n",
    "    o, r, d, ep_reward, ep_len, ep_num = env.reset(), 0, False, 0, 0, 1\n",
    "    \n",
    "    # track total steps\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    \n",
    "    for t in range(1,total_steps):\n",
    "        \n",
    "        explore = t < start_steps\n",
    "        a = agent.get_action(o, explore=explore)\n",
    "        \n",
    "        # Step the env\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_reward += r\n",
    "        ep_len += 1\n",
    "        \n",
    "        writer.add_scalar(\"reward_step\", r, t)\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time\n",
    "        # horizon (that is, when it's an artificial terminal signal\n",
    "        # that isn't based on the agent's state)\n",
    "        d = False if ep_len == max_ep_len else d\n",
    "\n",
    "        # Store experience to replay buffer\n",
    "        replay_buffer.push(o, a, r, o2, d)\n",
    "\n",
    "        # update observation\n",
    "        o = o2\n",
    "        \n",
    "        if d or (ep_len == max_ep_len):\n",
    "        \n",
    "            # carry out update for each step experienced (episode length)\n",
    "            if not explore:\n",
    "                agent.update(ep_len)\n",
    "            \n",
    "            # log progress\n",
    "            total_rewards.append(ep_reward)\n",
    "            avg_reward = np.mean(total_rewards[-10:])\n",
    "            \n",
    "            writer.add_scalar(\"avg_reward\", avg_reward, t)\n",
    "            writer.add_scalar(\"episode_reward\", ep_reward, t)\n",
    "            \n",
    "            print(\"Steps:{} Episode:{} Reward:{} Avg Reward:{}\".format(t, ep_num, ep_reward, avg_reward))\n",
    "            o, r, d, ep_reward, ep_len = env.reset(), 0, False, 0, 0\n",
    "            ep_num += 1\n",
    "            \n",
    "            if avg_reward > -50:\n",
    "                print(\"saving....\")\n",
    "                break\n",
    "\n",
    "        # End of epoch wrap-up\n",
    "        if t > 0 and t % steps_per_epoch == 0:\n",
    "            epoch = t // steps_per_epoch\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:200 Episode:1 Reward:-1427.0917955599123 Avg Reward:-1427.0917955599123\n",
      "Steps:400 Episode:2 Reward:-886.2195408825114 Avg Reward:-1156.6556682212117\n",
      "Steps:600 Episode:3 Reward:-1486.8925541287147 Avg Reward:-1266.7346301903792\n",
      "Steps:800 Episode:4 Reward:-1067.9983838134756 Avg Reward:-1217.0505685961534\n",
      "Steps:1000 Episode:5 Reward:-1662.3183718191065 Avg Reward:-1306.104129240744\n",
      "Steps:1200 Episode:6 Reward:-1578.10968520089 Avg Reward:-1351.4383885674351\n",
      "Steps:1400 Episode:7 Reward:-866.228313120975 Avg Reward:-1282.122663503655\n",
      "Steps:1600 Episode:8 Reward:-1604.0023937745216 Avg Reward:-1322.3576297875134\n",
      "Steps:1800 Episode:9 Reward:-1065.768460088575 Avg Reward:-1293.8477220431869\n",
      "Steps:2000 Episode:10 Reward:-1091.1967771112525 Avg Reward:-1273.5826275499935\n",
      "Steps:2200 Episode:11 Reward:-888.4377463418524 Avg Reward:-1219.7172226281875\n",
      "Steps:2400 Episode:12 Reward:-640.3823628178775 Avg Reward:-1195.133504821724\n",
      "Steps:2600 Episode:13 Reward:-1325.8349045463726 Avg Reward:-1179.02773986349\n",
      "Steps:2800 Episode:14 Reward:-131.35605806940453 Avg Reward:-1085.3635072890827\n",
      "Steps:3000 Episode:15 Reward:-5.225097573095395 Avg Reward:-919.6541798644815\n",
      "Steps:3200 Episode:16 Reward:-122.920381858879 Avg Reward:-774.1352495302806\n",
      "Steps:3400 Episode:17 Reward:-1.0808484746158808 Avg Reward:-687.6205030656447\n",
      "Steps:3600 Episode:18 Reward:-126.67885650791028 Avg Reward:-539.8881493389835\n",
      "Steps:3800 Episode:19 Reward:-247.3043193713185 Avg Reward:-458.0417352672579\n",
      "Steps:4000 Episode:20 Reward:-0.7919428386784424 Avg Reward:-349.00125184000046\n",
      "Steps:4200 Episode:21 Reward:-126.61003619816813 Avg Reward:-272.81848082563204\n",
      "Steps:4400 Episode:22 Reward:-357.61654316660616 Avg Reward:-244.5418988605049\n",
      "Steps:4600 Episode:23 Reward:-318.7606208281347 Avg Reward:-143.83447048868112\n",
      "Steps:4800 Episode:24 Reward:-228.5712208022882 Avg Reward:-153.55598676196948\n",
      "Steps:5000 Episode:25 Reward:-127.51533696759714 Avg Reward:-165.78501070141965\n",
      "Steps:5200 Episode:26 Reward:-226.7275078224345 Avg Reward:-176.1657232977752\n",
      "Steps:5400 Episode:27 Reward:-243.05449652363637 Avg Reward:-200.36308810267724\n",
      "Steps:5600 Episode:28 Reward:-121.52408724319825 Avg Reward:-199.84761117620604\n",
      "Steps:5800 Episode:29 Reward:-897.6379254129422 Avg Reward:-264.88097178036844\n",
      "Steps:6000 Episode:30 Reward:-344.3416622009601 Avg Reward:-299.2359437165966\n",
      "Steps:6200 Episode:31 Reward:-243.03189932798477 Avg Reward:-310.87813002957824\n",
      "Steps:6400 Episode:32 Reward:-240.7986727905627 Avg Reward:-299.1963429919739\n",
      "Steps:6600 Episode:33 Reward:-3.3333041162445474 Avg Reward:-267.6536113207849\n",
      "Steps:6800 Episode:34 Reward:-118.76936186415428 Avg Reward:-256.67342542697145\n",
      "Steps:7000 Episode:35 Reward:-230.34414005959428 Avg Reward:-266.9563057361712\n",
      "Steps:7200 Episode:36 Reward:-244.62125537876528 Avg Reward:-268.7456804918043\n",
      "Steps:7400 Episode:37 Reward:-235.98471760932313 Avg Reward:-268.03870260037297\n",
      "Steps:7600 Episode:38 Reward:-4.671444248675642 Avg Reward:-256.35343830092063\n",
      "Steps:7800 Episode:39 Reward:-123.51518080659248 Avg Reward:-178.94116384028572\n",
      "Steps:8000 Episode:40 Reward:-117.38830971169465 Avg Reward:-156.24582859135916\n",
      "Steps:8200 Episode:41 Reward:-249.36992824609266 Avg Reward:-156.87963148316996\n",
      "Steps:8400 Episode:42 Reward:-348.70235922048903 Avg Reward:-167.6700001261626\n",
      "Steps:8600 Episode:43 Reward:-125.13533539577072 Avg Reward:-179.8502032541152\n",
      "Steps:8800 Episode:44 Reward:-1.9767122437212248 Avg Reward:-168.1709382920719\n",
      "Steps:9000 Episode:45 Reward:-124.77515688466872 Avg Reward:-157.61403997457938\n",
      "Steps:9200 Episode:46 Reward:-121.05826451774603 Avg Reward:-145.25774088847743\n",
      "Steps:9400 Episode:47 Reward:-116.80813737998945 Avg Reward:-133.34008286554405\n",
      "Steps:9600 Episode:48 Reward:-120.25978106501024 Avg Reward:-144.8989165471775\n",
      "Steps:9800 Episode:49 Reward:-3.232454201524891 Avg Reward:-132.87064388667073\n",
      "Steps:10000 Episode:50 Reward:-0.7275202078379045 Avg Reward:-121.20456493628508\n",
      "Steps:10200 Episode:51 Reward:-230.90387566994158 Avg Reward:-119.35795967866997\n",
      "Steps:10400 Episode:52 Reward:-122.68476992626026 Avg Reward:-96.7562007492471\n",
      "Steps:10600 Episode:53 Reward:-116.58952495258316 Avg Reward:-95.90161970492834\n",
      "Steps:10800 Episode:54 Reward:-119.82442650756734 Avg Reward:-107.68639113131296\n",
      "Steps:11000 Episode:55 Reward:-122.27651355544265 Avg Reward:-107.43652679839033\n",
      "Steps:11200 Episode:56 Reward:-232.32522823202402 Avg Reward:-118.56322316981814\n",
      "Steps:11400 Episode:57 Reward:-125.91172936996031 Avg Reward:-119.47358236881523\n",
      "Steps:11600 Episode:58 Reward:-122.55715440079555 Avg Reward:-119.70331970239377\n",
      "Steps:11800 Episode:59 Reward:-119.56675383955663 Avg Reward:-131.33674966619694\n",
      "Steps:12000 Episode:60 Reward:-114.85385285967116 Avg Reward:-142.74938293138027\n",
      "Steps:12200 Episode:61 Reward:-125.54711823942041 Avg Reward:-132.21370718832816\n",
      "Steps:12400 Episode:62 Reward:-119.76800911455699 Avg Reward:-131.9220311071578\n",
      "Steps:12600 Episode:63 Reward:-115.20151195609012 Avg Reward:-131.78322980750852\n",
      "Steps:12800 Episode:64 Reward:-118.70677308339639 Avg Reward:-131.67146446509145\n",
      "Steps:13000 Episode:65 Reward:-114.7756472689899 Avg Reward:-130.92137783644617\n",
      "Steps:13200 Episode:66 Reward:-122.0757750780899 Avg Reward:-119.89643252105273\n",
      "Steps:13400 Episode:67 Reward:-1.363016093261702 Avg Reward:-107.44156119338285\n",
      "Steps:13600 Episode:68 Reward:-235.11975829777558 Avg Reward:-118.69782158308087\n",
      "Steps:13800 Episode:69 Reward:-228.63251521093048 Avg Reward:-129.6043977202183\n",
      "Steps:14000 Episode:70 Reward:-117.32405288435264 Avg Reward:-129.8514177226864\n",
      "Steps:14200 Episode:71 Reward:-120.8182511078871 Avg Reward:-129.37853100953308\n",
      "Steps:14400 Episode:72 Reward:-126.12962189486306 Avg Reward:-130.0146922875637\n",
      "Steps:14600 Episode:73 Reward:-226.99305601144334 Avg Reward:-141.193846693099\n",
      "Steps:14800 Episode:74 Reward:-2.3706222375483783 Avg Reward:-129.5602316085142\n",
      "Steps:15000 Episode:75 Reward:-115.63051373090418 Avg Reward:-129.64571825470563\n",
      "Steps:15200 Episode:76 Reward:-2.2504319559803263 Avg Reward:-117.66318394249465\n",
      "Steps:15400 Episode:77 Reward:-115.25406415079212 Avg Reward:-129.0522887482477\n",
      "Steps:15600 Episode:78 Reward:-123.37917869423879 Avg Reward:-117.87823078789404\n",
      "Steps:15800 Episode:79 Reward:-118.31384807010078 Avg Reward:-106.84636407381106\n",
      "Steps:16000 Episode:80 Reward:-126.01200846952203 Avg Reward:-107.71515963232802\n",
      "Steps:16200 Episode:81 Reward:-251.56197665144026 Avg Reward:-120.78953218668332\n",
      "Steps:16400 Episode:82 Reward:-2.2626895218389556 Avg Reward:-108.40283894938091\n",
      "Steps:16600 Episode:83 Reward:-341.3675993454439 Avg Reward:-119.84029328278098\n",
      "Steps:16800 Episode:84 Reward:-130.83382226952043 Avg Reward:-132.68661328597818\n",
      "Steps:17000 Episode:85 Reward:-121.93168277750206 Avg Reward:-133.31673019063797\n",
      "Steps:17200 Episode:86 Reward:-123.22329595473434 Avg Reward:-145.41401659051337\n",
      "Steps:17400 Episode:87 Reward:-342.94127530384867 Avg Reward:-168.18273770581902\n",
      "Steps:17600 Episode:88 Reward:-117.87989117365608 Avg Reward:-167.63280895376073\n",
      "Steps:17800 Episode:89 Reward:-126.56399953903549 Avg Reward:-168.45782410065422\n",
      "Steps:18000 Episode:90 Reward:-119.33131393609874 Avg Reward:-167.7897546473119\n",
      "Steps:18200 Episode:91 Reward:-124.94559728457646 Avg Reward:-155.1281167106255\n",
      "Steps:18400 Episode:92 Reward:-119.61802753688649 Avg Reward:-166.86365051213028\n",
      "Steps:18600 Episode:93 Reward:-116.47760636130221 Avg Reward:-144.3746512137161\n",
      "Steps:18800 Episode:94 Reward:-126.0972316041214 Avg Reward:-143.90099214717617\n",
      "Steps:19000 Episode:95 Reward:-1.2793858893809527 Avg Reward:-131.8357624583641\n",
      "Steps:19200 Episode:96 Reward:-246.2298919644187 Avg Reward:-144.13642205933252\n",
      "Steps:19400 Episode:97 Reward:-122.82000687088576 Avg Reward:-122.12429521603622\n",
      "Steps:19600 Episode:98 Reward:-117.70899704414748 Avg Reward:-122.10720580308536\n",
      "Steps:19800 Episode:99 Reward:-119.77209258526743 Avg Reward:-121.42801510770857\n",
      "Steps:20000 Episode:100 Reward:-227.36015373868995 Avg Reward:-132.23089908796769\n",
      "Steps:20200 Episode:101 Reward:-118.66221360673893 Avg Reward:-131.60256072018393\n",
      "Steps:20400 Episode:102 Reward:-116.58890763963872 Avg Reward:-131.29964873045915\n",
      "Steps:20600 Episode:103 Reward:-240.40254143014167 Avg Reward:-143.6921422373431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:20800 Episode:104 Reward:-237.5071764164259 Avg Reward:-154.8331367185735\n",
      "Steps:21000 Episode:105 Reward:-240.32777087117452 Avg Reward:-178.7379752167529\n",
      "Steps:21200 Episode:106 Reward:-120.29701253363616 Avg Reward:-166.14468727367466\n",
      "Steps:21400 Episode:107 Reward:-119.60980908426764 Avg Reward:-165.82366749501284\n",
      "Steps:21600 Episode:108 Reward:-229.1542630891135 Avg Reward:-176.96819409950945\n",
      "Steps:21800 Episode:109 Reward:-118.93596002173055 Avg Reward:-176.88458084315576\n",
      "Steps:22000 Episode:110 Reward:-237.1052057699421 Avg Reward:-177.859086046281\n",
      "Steps:22200 Episode:111 Reward:-128.16329005887312 Avg Reward:-178.8091936914944\n",
      "Steps:22400 Episode:112 Reward:-127.123431699418 Avg Reward:-179.86264609747232\n",
      "Steps:22600 Episode:113 Reward:-229.59276121663922 Avg Reward:-178.78166807612206\n",
      "Steps:22800 Episode:114 Reward:-119.3299102811465 Avg Reward:-166.96394146259414\n",
      "Steps:23000 Episode:115 Reward:-118.27671818817059 Avg Reward:-154.75883619429376\n",
      "Steps:23200 Episode:116 Reward:-222.83730265474145 Avg Reward:-165.01286520640426\n",
      "Steps:23400 Episode:117 Reward:-235.98899993111624 Avg Reward:-176.6507842910891\n",
      "Steps:23600 Episode:118 Reward:-126.2407961430048 Avg Reward:-166.35943759647824\n",
      "Steps:23800 Episode:119 Reward:-117.54629220494793 Avg Reward:-166.22047081480002\n",
      "Steps:24000 Episode:120 Reward:-125.45529752833787 Avg Reward:-155.0554799906396\n",
      "Steps:24200 Episode:121 Reward:-235.80460337789313 Avg Reward:-165.81961132254156\n",
      "Steps:24400 Episode:122 Reward:-118.9372525826146 Avg Reward:-165.0009934108612\n",
      "Steps:24600 Episode:123 Reward:-117.961998959686 Avg Reward:-153.8379171851659\n",
      "Steps:24800 Episode:124 Reward:-245.3856918071289 Avg Reward:-166.44349533776418\n",
      "Steps:25000 Episode:125 Reward:-123.12342369044586 Avg Reward:-166.9281658879917\n",
      "Steps:25200 Episode:126 Reward:-246.32523579608613 Avg Reward:-169.27695920212616\n",
      "Steps:25400 Episode:127 Reward:-125.66411572368978 Avg Reward:-158.2444707813835\n",
      "Steps:25600 Episode:128 Reward:-125.66958562928983 Avg Reward:-158.18734973001202\n",
      "Steps:25800 Episode:129 Reward:-120.84247925934801 Avg Reward:-158.51696843545204\n",
      "Steps:26000 Episode:130 Reward:-220.6336492221315 Avg Reward:-168.03480360483135\n",
      "Steps:26200 Episode:131 Reward:-1.0626292424512116 Avg Reward:-144.56060619128718\n",
      "Steps:26400 Episode:132 Reward:-229.80393988671278 Avg Reward:-155.647274921697\n",
      "Steps:26600 Episode:133 Reward:-125.46389038739781 Avg Reward:-156.3974640644682\n",
      "Steps:26800 Episode:134 Reward:-116.94849307602037 Avg Reward:-143.55374419135734\n",
      "Steps:27000 Episode:135 Reward:-242.0831710552855 Avg Reward:-155.4497189278413\n",
      "Steps:27200 Episode:136 Reward:-247.5390537360096 Avg Reward:-155.57110072183363\n",
      "Steps:27400 Episode:137 Reward:-116.76708424820647 Avg Reward:-154.6813975742853\n",
      "Steps:27600 Episode:138 Reward:-226.4910582796975 Avg Reward:-164.76354483932607\n",
      "Steps:27800 Episode:139 Reward:-119.63380986919799 Avg Reward:-164.64267790031107\n",
      "Steps:28000 Episode:140 Reward:-231.1177609131766 Avg Reward:-165.69108906941563\n",
      "Steps:28200 Episode:141 Reward:-117.7121380577789 Avg Reward:-177.35603995094837\n",
      "Steps:28400 Episode:142 Reward:-227.34372956132896 Avg Reward:-177.11001891840996\n",
      "Steps:28600 Episode:143 Reward:-247.13177292163533 Avg Reward:-189.2768071718337\n",
      "Steps:28800 Episode:144 Reward:-119.78405704533851 Avg Reward:-189.56036356876555\n",
      "Steps:29000 Episode:145 Reward:-120.0487637962844 Avg Reward:-177.35692284286546\n",
      "Steps:29200 Episode:146 Reward:-122.00479687181263 Avg Reward:-164.80349715644576\n",
      "Steps:29400 Episode:147 Reward:-1.0001404496631117 Avg Reward:-153.2268027765914\n",
      "Steps:29600 Episode:148 Reward:-120.73193985511183 Avg Reward:-142.65089093413283\n",
      "Steps:29800 Episode:149 Reward:-3.134671712952927 Avg Reward:-131.00097711850833\n",
      "Steps:30000 Episode:150 Reward:-226.0758687169349 Avg Reward:-130.49678789888418\n",
      "Steps:30200 Episode:151 Reward:-237.82511025653483 Avg Reward:-142.50808511875974\n",
      "Steps:30400 Episode:152 Reward:-230.67483580599279 Avg Reward:-142.84119574322614\n",
      "Steps:30600 Episode:153 Reward:-1.6151545301546886 Avg Reward:-118.28953390407807\n",
      "Steps:30800 Episode:154 Reward:-115.33623847461016 Avg Reward:-117.84475204700523\n",
      "Steps:31000 Episode:155 Reward:-120.24811852669856 Avg Reward:-117.86468752004664\n",
      "Steps:31200 Episode:156 Reward:-233.556681510037 Avg Reward:-129.01987598386907\n",
      "Steps:31400 Episode:157 Reward:-115.03465846018189 Avg Reward:-140.42332778492093\n",
      "Steps:31600 Episode:158 Reward:-126.0118107613295 Avg Reward:-140.95131487554272\n",
      "Steps:31800 Episode:159 Reward:-116.30405266882971 Avg Reward:-152.26825297113038\n",
      "Steps:32000 Episode:160 Reward:-123.74147051312387 Avg Reward:-142.0348131507493\n",
      "Steps:32200 Episode:161 Reward:-218.59672825361378 Avg Reward:-140.11197495045718\n",
      "Steps:32400 Episode:162 Reward:-120.88636666004501 Avg Reward:-129.1331280358624\n",
      "Steps:32600 Episode:163 Reward:-330.78175112791826 Avg Reward:-162.04978769563877\n",
      "Steps:32800 Episode:164 Reward:-120.59168209372545 Avg Reward:-162.5753320575503\n",
      "Steps:33000 Episode:165 Reward:-122.44283075266657 Avg Reward:-162.79480328014714\n",
      "Steps:33200 Episode:166 Reward:-124.16828392740815 Avg Reward:-151.8559635218842\n",
      "Steps:33400 Episode:167 Reward:-120.6639862538122 Avg Reward:-152.41889630124723\n",
      "Steps:33600 Episode:168 Reward:-345.84965896477337 Avg Reward:-174.40268112159166\n",
      "Steps:33800 Episode:169 Reward:-121.78906409525736 Avg Reward:-174.95118226423443\n",
      "Steps:34000 Episode:170 Reward:-120.36142180139328 Avg Reward:-174.61317739306133\n",
      "Steps:34200 Episode:171 Reward:-225.68550274839055 Avg Reward:-175.32205484253902\n",
      "Steps:34400 Episode:172 Reward:-123.58394393988478 Avg Reward:-175.59181257052302\n",
      "Steps:34600 Episode:173 Reward:-123.08222871263035 Avg Reward:-154.8218603289942\n",
      "Steps:34800 Episode:174 Reward:-306.0613205029651 Avg Reward:-173.36882416991818\n",
      "Steps:35000 Episode:175 Reward:-119.33629148929957 Avg Reward:-173.0581702435815\n",
      "Steps:35200 Episode:176 Reward:-225.94638213343072 Avg Reward:-183.2359800641837\n",
      "Steps:35400 Episode:177 Reward:-3.968158345896213 Avg Reward:-171.56639727339214\n",
      "Steps:35600 Episode:178 Reward:-245.8296384458515 Avg Reward:-161.56439522149995\n",
      "Steps:35800 Episode:179 Reward:-123.56922788333253 Avg Reward:-161.74241160030746\n",
      "Steps:36000 Episode:180 Reward:-244.22749998877524 Avg Reward:-174.12901941904565\n",
      "Steps:36200 Episode:181 Reward:-349.5405982915562 Avg Reward:-186.51452897336222\n",
      "Steps:36400 Episode:182 Reward:-118.96193806736277 Avg Reward:-186.05232838611002\n",
      "Steps:36600 Episode:183 Reward:-117.5803341056073 Avg Reward:-185.5021389254077\n",
      "Steps:36800 Episode:184 Reward:-120.46911649734398 Avg Reward:-166.94291852484562\n",
      "Steps:37000 Episode:185 Reward:-225.34293625097555 Avg Reward:-177.5435830010132\n",
      "Steps:37200 Episode:186 Reward:-123.6367196390943 Avg Reward:-167.31261675157958\n",
      "Steps:37400 Episode:187 Reward:-120.97227385065112 Avg Reward:-179.01302830205503\n",
      "Steps:37600 Episode:188 Reward:-2.940125113124373 Avg Reward:-154.72407696878236\n",
      "Steps:37800 Episode:189 Reward:-244.24799548430428 Avg Reward:-166.7919537288795\n",
      "Steps:38000 Episode:190 Reward:-1.5197957962670603 Avg Reward:-142.5211833096287\n",
      "Steps:38200 Episode:191 Reward:-117.08385190598851 Avg Reward:-119.27550867107193\n",
      "Steps:38400 Episode:192 Reward:-221.60799393532096 Avg Reward:-129.54011425786774\n",
      "Steps:38600 Episode:193 Reward:-117.13829780553172 Avg Reward:-129.49591062786018\n",
      "Steps:38800 Episode:194 Reward:-310.4125295590765 Avg Reward:-148.49025193403344\n",
      "Steps:39000 Episode:195 Reward:-127.41749551871949 Avg Reward:-138.69770786080784\n",
      "Steps:39200 Episode:196 Reward:-247.06793126319909 Avg Reward:-151.04082902321835\n",
      "Steps:39400 Episode:197 Reward:-124.03912248032807 Avg Reward:-151.34751388618605\n",
      "Steps:39600 Episode:198 Reward:-228.84219121304125 Avg Reward:-173.93772049617775\n",
      "Steps:39800 Episode:199 Reward:-230.3153643018277 Avg Reward:-172.54445737793003\n",
      "Steps:40000 Episode:200 Reward:-128.22347101440803 Avg Reward:-185.21482489974414\n",
      "Steps:40200 Episode:201 Reward:-237.64231486556932 Avg Reward:-197.27067119570222\n",
      "Steps:40400 Episode:202 Reward:-118.59844895350517 Avg Reward:-186.9697166975206\n",
      "Steps:40600 Episode:203 Reward:-123.99147254166844 Avg Reward:-187.6550341711343\n",
      "Steps:40800 Episode:204 Reward:-120.23982794075218 Avg Reward:-168.63776400930186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:41000 Episode:205 Reward:-121.01087826659898 Avg Reward:-167.9971022840898\n",
      "Steps:41200 Episode:206 Reward:-125.28891611312274 Avg Reward:-155.81920076908216\n",
      "Steps:41400 Episode:207 Reward:-125.78569875433439 Avg Reward:-155.9938583964828\n",
      "Steps:41600 Episode:208 Reward:-119.56564881247616 Avg Reward:-145.06620415642632\n",
      "Steps:41800 Episode:209 Reward:-117.8243728136476 Avg Reward:-133.8171050076083\n",
      "Steps:42000 Episode:210 Reward:-1.4310259329983965 Avg Reward:-121.13786049946734\n",
      "Steps:42200 Episode:211 Reward:-122.51224994866664 Avg Reward:-109.62485400777707\n",
      "Steps:42400 Episode:212 Reward:-117.59813865783114 Avg Reward:-109.52482297820966\n",
      "Steps:42600 Episode:213 Reward:-120.86666327141572 Avg Reward:-109.21234205118438\n",
      "Steps:42800 Episode:214 Reward:-1.7651033502310498 Avg Reward:-97.3648695921323\n",
      "Steps:43000 Episode:215 Reward:-348.5586369760247 Avg Reward:-120.11964546307486\n",
      "Steps:43200 Episode:216 Reward:-0.40493637690588996 Avg Reward:-107.63124748945316\n",
      "Steps:43400 Episode:217 Reward:-3.7852932757237747 Avg Reward:-95.4312069415921\n",
      "Steps:43600 Episode:218 Reward:-118.74462211769999 Avg Reward:-95.34910427211449\n",
      "Steps:43800 Episode:219 Reward:-236.54061029963574 Avg Reward:-107.2207280207133\n",
      "Steps:44000 Episode:220 Reward:-238.4794628347468 Avg Reward:-130.92557171088814\n",
      "Steps:44200 Episode:221 Reward:-120.1038768531503 Avg Reward:-130.6847344013365\n",
      "Steps:44400 Episode:222 Reward:-122.87135286803972 Avg Reward:-131.21205582235737\n",
      "Steps:44600 Episode:223 Reward:-2.46947121241666 Avg Reward:-119.37233661645749\n",
      "Steps:44800 Episode:224 Reward:-234.23913023728844 Avg Reward:-142.61973930516322\n",
      "Steps:45000 Episode:225 Reward:-224.91739102027452 Avg Reward:-130.2556147095882\n",
      "Steps:45200 Episode:226 Reward:-129.44635967791518 Avg Reward:-143.1597570396891\n",
      "Steps:45400 Episode:227 Reward:-360.259707091055 Avg Reward:-178.80719842122227\n",
      "Steps:45600 Episode:228 Reward:-121.64391911942347 Avg Reward:-179.0971281213946\n",
      "Steps:45800 Episode:229 Reward:-242.45080642568445 Avg Reward:-179.68814773399947\n",
      "Steps:46000 Episode:230 Reward:-2.9040419476833166 Avg Reward:-156.13060564529312\n",
      "Steps:46200 Episode:231 Reward:-113.97312269478968 Avg Reward:-155.51753022945704\n",
      "Steps:46400 Episode:232 Reward:-123.54713098999798 Avg Reward:-155.5851080416529\n",
      "Steps:46600 Episode:233 Reward:-121.67685478925533 Avg Reward:-167.50584639933672\n",
      "Steps:46800 Episode:234 Reward:-2.855811140566208 Avg Reward:-144.3675144896645\n",
      "Steps:47000 Episode:235 Reward:-248.68988233418656 Avg Reward:-146.7447636210557\n",
      "Steps:47200 Episode:236 Reward:-331.75858623035003 Avg Reward:-166.97598627629918\n",
      "Steps:47400 Episode:237 Reward:-121.68619159467094 Avg Reward:-143.1186347266608\n",
      "Steps:47600 Episode:238 Reward:-126.60168267682215 Avg Reward:-143.61441108240064\n",
      "Steps:47800 Episode:239 Reward:-118.02290084077978 Avg Reward:-131.17162052391018\n",
      "Steps:48000 Episode:240 Reward:-113.73973297859274 Avg Reward:-142.25518962700113\n",
      "Steps:48200 Episode:241 Reward:-123.78958761411118 Avg Reward:-143.23683611893327\n",
      "Steps:48400 Episode:242 Reward:-296.6091116330478 Avg Reward:-160.5430341832383\n",
      "Steps:48600 Episode:243 Reward:-125.67291444299049 Avg Reward:-160.94264014861182\n",
      "Steps:48800 Episode:244 Reward:-119.75065299840536 Avg Reward:-172.6321243343957\n",
      "Steps:49000 Episode:245 Reward:-371.74849062083103 Avg Reward:-184.93798516306015\n",
      "Steps:49200 Episode:246 Reward:-2.699268137653933 Avg Reward:-152.03205335379056\n",
      "Steps:49400 Episode:247 Reward:-242.16678741949698 Avg Reward:-164.08011293627314\n",
      "Steps:49600 Episode:248 Reward:-226.1649861020059 Avg Reward:-174.03644327879152\n",
      "Steps:49800 Episode:249 Reward:-263.29019852717187 Avg Reward:-188.56317304743075\n",
      "Steps:50000 Episode:250 Reward:-123.91230941523392 Avg Reward:-189.58043069109488\n",
      "Steps:50200 Episode:251 Reward:-120.41603053089965 Avg Reward:-189.2430749827737\n",
      "Steps:50400 Episode:252 Reward:-226.36587238197453 Avg Reward:-182.21875105766634\n",
      "Steps:50600 Episode:253 Reward:-124.699887365021 Avg Reward:-182.12144834986938\n",
      "Steps:50800 Episode:254 Reward:-246.94797270062898 Avg Reward:-194.84118032009175\n",
      "Steps:51000 Episode:255 Reward:-123.8420244975616 Avg Reward:-170.05053370776483\n",
      "Steps:51200 Episode:256 Reward:-220.94197430876068 Avg Reward:-191.8748043248755\n",
      "Steps:51400 Episode:257 Reward:-115.39139044589027 Avg Reward:-179.19726462751484\n",
      "Steps:51600 Episode:258 Reward:-121.7541724902265 Avg Reward:-168.7561832663369\n",
      "Steps:51800 Episode:259 Reward:-121.55250107245524 Avg Reward:-154.58241352086523\n",
      "Steps:52000 Episode:260 Reward:-234.68883102955203 Avg Reward:-165.66006568229704\n",
      "Steps:52200 Episode:261 Reward:-121.05428373276415 Avg Reward:-165.7238910024835\n",
      "Steps:52400 Episode:262 Reward:-231.89575151596364 Avg Reward:-166.2768789158824\n",
      "Steps:52600 Episode:263 Reward:-124.653267706697 Avg Reward:-166.27221695005\n",
      "Steps:52800 Episode:264 Reward:-1.6868694235743207 Avg Reward:-141.74610662234457\n",
      "Steps:53000 Episode:265 Reward:-117.70782315941844 Avg Reward:-141.13268648853023\n",
      "Steps:53200 Episode:266 Reward:-117.12420733021749 Avg Reward:-130.75090979067593\n",
      "Steps:53400 Episode:267 Reward:-225.12735100766642 Avg Reward:-141.72450584685353\n",
      "Steps:53600 Episode:268 Reward:-118.82878289805322 Avg Reward:-141.4319668876362\n",
      "Steps:53800 Episode:269 Reward:-123.51740218732247 Avg Reward:-141.62845699912293\n",
      "Steps:54000 Episode:270 Reward:-124.62216988716445 Avg Reward:-130.62179088488418\n",
      "Steps:54200 Episode:271 Reward:-342.51448638149884 Avg Reward:-152.76781114975765\n",
      "Steps:54400 Episode:272 Reward:-0.8167053762147156 Avg Reward:-129.6599065357827\n",
      "Steps:54600 Episode:273 Reward:-121.56941397415441 Avg Reward:-129.3515211625285\n",
      "Steps:54800 Episode:274 Reward:-116.37622623065035 Avg Reward:-140.8204568432361\n",
      "Steps:55000 Episode:275 Reward:-121.60388320039897 Avg Reward:-141.21006284733414\n",
      "Steps:55200 Episode:276 Reward:-124.82976116993912 Avg Reward:-141.9806182313063\n",
      "Steps:55400 Episode:277 Reward:-115.42964226397855 Avg Reward:-131.01084735693752\n",
      "Steps:55600 Episode:278 Reward:-119.68857751233203 Avg Reward:-131.0968268183654\n",
      "Steps:55800 Episode:279 Reward:-372.6045660968238 Avg Reward:-156.00554320931553\n",
      "Steps:56000 Episode:280 Reward:-118.90889957888173 Avg Reward:-155.43421617848728\n",
      "Steps:56200 Episode:281 Reward:-123.23379687974 Avg Reward:-133.50614722831136\n",
      "Steps:56400 Episode:282 Reward:-119.31462214285347 Avg Reward:-145.3559389049753\n",
      "Steps:56600 Episode:283 Reward:-124.2491080561437 Avg Reward:-145.62390831317418\n",
      "Steps:56800 Episode:284 Reward:-115.38579093997001 Avg Reward:-145.52486478410614\n",
      "Steps:57000 Episode:285 Reward:-126.18357035821928 Avg Reward:-145.98283349988816\n",
      "Steps:57200 Episode:286 Reward:-128.13286130244214 Avg Reward:-146.31314351313844\n",
      "Steps:57400 Episode:287 Reward:-123.10834509501144 Avg Reward:-147.08101379624173\n",
      "Steps:57600 Episode:288 Reward:-241.6790899662745 Avg Reward:-159.280065041636\n",
      "Steps:57800 Episode:289 Reward:-130.50839462597125 Avg Reward:-135.07044789455077\n",
      "Steps:58000 Episode:290 Reward:-5.187317072791025 Avg Reward:-123.69828964394169\n",
      "Steps:58200 Episode:291 Reward:-120.48210012767392 Avg Reward:-123.4231199687351\n",
      "Steps:58400 Episode:292 Reward:-220.92779834711075 Avg Reward:-133.58443758916079\n",
      "Steps:58600 Episode:293 Reward:-113.65759981692746 Avg Reward:-132.52528676523917\n",
      "Steps:58800 Episode:294 Reward:-0.7623607106528931 Avg Reward:-121.06294374230745\n",
      "Steps:59000 Episode:295 Reward:-127.2119954568066 Avg Reward:-121.16578625216621\n",
      "Steps:59200 Episode:296 Reward:-114.21127751941957 Avg Reward:-119.77362787386394\n",
      "Steps:59400 Episode:297 Reward:-1.355533862254166 Avg Reward:-107.5983467505882\n",
      "Steps:59600 Episode:298 Reward:-233.4738188891192 Avg Reward:-106.77781964287267\n",
      "Steps:59800 Episode:299 Reward:-118.25646626893703 Avg Reward:-105.55262680716926\n",
      "Steps:60000 Episode:300 Reward:-114.51199885429202 Avg Reward:-116.48509498531935\n",
      "Steps:60200 Episode:301 Reward:-122.5085471120448 Avg Reward:-116.68773968375646\n",
      "Steps:60400 Episode:302 Reward:-220.3342734060258 Avg Reward:-116.62838718964795\n",
      "Steps:60600 Episode:303 Reward:-114.93521513006594 Avg Reward:-116.7561487209618\n",
      "Steps:60800 Episode:304 Reward:-120.6255955841775 Avg Reward:-128.74247220831427\n",
      "Steps:61000 Episode:305 Reward:-237.5610298663616 Avg Reward:-139.77737564926977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:61200 Episode:306 Reward:-120.74751136510478 Avg Reward:-140.4309990338383\n",
      "Steps:61400 Episode:307 Reward:-122.4719340888489 Avg Reward:-152.54263905649776\n",
      "Steps:61600 Episode:308 Reward:-1.4348082668576012 Avg Reward:-129.33873799427158\n",
      "Steps:61800 Episode:309 Reward:-118.99027662921945 Avg Reward:-129.41211903029983\n",
      "Steps:62000 Episode:310 Reward:-126.52376121508688 Avg Reward:-130.6132952663793\n",
      "Steps:62200 Episode:311 Reward:-124.13105891199771 Avg Reward:-130.77554644637462\n",
      "Steps:62400 Episode:312 Reward:-119.81702480893908 Avg Reward:-120.72382158666593\n",
      "Steps:62600 Episode:313 Reward:-230.86645346411487 Avg Reward:-132.31694542007082\n",
      "Steps:62800 Episode:314 Reward:-125.32058024139246 Avg Reward:-132.78644388579232\n",
      "Steps:63000 Episode:315 Reward:-227.60245507077605 Avg Reward:-131.79058640623376\n",
      "Steps:63200 Episode:316 Reward:-0.8794729470805196 Avg Reward:-119.80378256443137\n",
      "Steps:63400 Episode:317 Reward:-122.0237468754695 Avg Reward:-119.75896384309343\n",
      "Steps:63600 Episode:318 Reward:-125.54318301409188 Avg Reward:-132.16980131781685\n",
      "Steps:63800 Episode:319 Reward:-3.6872827866896087 Avg Reward:-120.63950193356386\n",
      "Steps:64000 Episode:320 Reward:-224.53575714268402 Avg Reward:-130.44070152632358\n",
      "Steps:64200 Episode:321 Reward:-126.59939764984824 Avg Reward:-130.68753540010863\n",
      "Steps:64400 Episode:322 Reward:-121.14329588305908 Avg Reward:-130.82016250752065\n",
      "Steps:64600 Episode:323 Reward:-122.62768212446208 Avg Reward:-119.99628537355537\n",
      "Steps:64800 Episode:324 Reward:-241.0173053104575 Avg Reward:-131.56595788046184\n",
      "Steps:65000 Episode:325 Reward:-121.08959691657978 Avg Reward:-120.91467206504221\n",
      "Steps:65200 Episode:326 Reward:-125.95706939745423 Avg Reward:-133.4224317100796\n",
      "Steps:65400 Episode:327 Reward:-313.756457864084 Avg Reward:-152.59570280894104\n",
      "Steps:65600 Episode:328 Reward:-0.44243207281515345 Avg Reward:-140.08562771481337\n",
      "Steps:65800 Episode:329 Reward:-3.907544231382406 Avg Reward:-140.10765385928264\n",
      "Steps:66000 Episode:330 Reward:-129.45367412382873 Avg Reward:-130.59944555739713\n",
      "Steps:66200 Episode:331 Reward:-120.42573834556399 Avg Reward:-129.9820796269687\n",
      "Steps:66400 Episode:332 Reward:-125.0219437213975 Avg Reward:-130.36994441080256\n",
      "Steps:66600 Episode:333 Reward:-118.50635380199796 Avg Reward:-129.95781157855612\n",
      "Steps:66800 Episode:334 Reward:-117.88610950272702 Avg Reward:-117.6446919977831\n",
      "Steps:67000 Episode:335 Reward:-121.66125970875143 Avg Reward:-117.70185827700024\n",
      "Steps:67200 Episode:336 Reward:-124.96103751585953 Avg Reward:-117.60225508884078\n",
      "Steps:67400 Episode:337 Reward:-113.18580933150517 Avg Reward:-97.5451902355829\n",
      "Steps:67600 Episode:338 Reward:-120.12275448059668 Avg Reward:-109.51322247636105\n",
      "Steps:67800 Episode:339 Reward:-115.5553076124047 Avg Reward:-120.67799881446328\n",
      "Steps:68000 Episode:340 Reward:-239.64616026593097 Avg Reward:-131.6972474286735\n",
      "Steps:68200 Episode:341 Reward:-223.9373757033552 Avg Reward:-142.0484111644526\n",
      "Steps:68400 Episode:342 Reward:-127.69936207567633 Avg Reward:-142.3161529998805\n",
      "Steps:68600 Episode:343 Reward:-227.97670668021627 Avg Reward:-153.26318828770235\n",
      "Steps:68800 Episode:344 Reward:-223.22445153668653 Avg Reward:-163.7970224910983\n",
      "Steps:69000 Episode:345 Reward:-117.20396812269897 Avg Reward:-163.35129333249304\n",
      "Steps:69200 Episode:346 Reward:-5.090899095121995 Avg Reward:-151.36427949041928\n",
      "Steps:69400 Episode:347 Reward:-233.02483850104989 Avg Reward:-163.34818240737377\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7fe0f2965a0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-cf5157e7b87e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, steps_per_epoch, epochs, start_steps, max_ep_len)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# carry out update for each step experienced (episode length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexplore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# log progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-c8d7162a742e>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iterations, batch_size)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl_dev/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl_dev/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = \"Pendulum-v0\"#\"RoboschoolHalfCheetah-v1\"\n",
    "\n",
    "replay_buffer = ReplayBuffer(int(1e6))\n",
    "\n",
    "env = NormalizedActions(gym.make(env))\n",
    "\n",
    "agent = SAC(env, replay_buffer)\n",
    "\n",
    "train(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
