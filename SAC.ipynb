{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from tensorboardX import SummaryWriter\n",
    "import roboschool\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.linear3 = nn.Linear(hidden_size[1], 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], \n",
    "                 init_w=3e-3, log_std_min=-20, log_std_max=2, epsilon=1e-6):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        \n",
    "        self.mean_linear = nn.Linear(hidden_size[1], num_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "        self.log_std_linear = nn.Linear(hidden_size[1], num_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def rsample(self, return_pretanh_value=False):\n",
    "        \"\"\"\n",
    "        Sampling in the reparameterization case.\n",
    "        \"\"\"\n",
    "        z = (\n",
    "            self.normal_mean +\n",
    "            self.normal_std *\n",
    "            Normal(\n",
    "                ptu.zeros(self.normal_mean.size()),\n",
    "                ptu.ones(self.normal_std.size())\n",
    "            ).sample()\n",
    "        )\n",
    "        z.requires_grad_()\n",
    "\n",
    "        if return_pretanh_value:\n",
    "            return torch.tanh(z), z\n",
    "        else:\n",
    "            return torch.tanh(z)\n",
    "    \n",
    "    def forward(self, state, deterministic=False):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        mean    = self.mean_linear(x)\n",
    "        \n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        log_prob = None\n",
    "        \n",
    "        if deterministic:\n",
    "            action = torch.tanh(mean)\n",
    "        else:\n",
    "            # assumes actions have been normalized to (0,1)\n",
    "            normal = Normal(0, 1)\n",
    "            z = mean + std * normal.sample().requires_grad_()\n",
    "            action = torch.tanh(z)\n",
    "            log_prob = Normal(mean, std).log_prob(z) - torch.log(1 - action * action + self.epsilon)\n",
    "            \n",
    "        return action, mean, log_std, log_prob, std\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action,_,_,_,_ =  self.forward(state, deterministic)\n",
    "        act = action.cpu()[0][0]\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.linear3 = nn.Linear(hidden_size[1], 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        \n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "def normalize_action(action, low, high):\n",
    "    action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "    action = np.clip(action, low, high)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(object):\n",
    "    \n",
    "    def __init__(self, env, replay_buffer, seed=0, hidden_dim=[400,300],\n",
    "        steps_per_epoch=200, epochs=1000, discount=0.99,\n",
    "        tau=1e-2, lr=1e-3, auto_alpha=True, batch_size=100, start_steps=10000,\n",
    "        max_ep_len=200, logger_kwargs=dict(), save_freq=1):\n",
    "        \n",
    "        # Set seeds\n",
    "        self.env = env\n",
    "        self.env.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # env space\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0] \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # init networks\n",
    "        \n",
    "        # Soft Q\n",
    "        self.soft_q_net1 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        self.soft_q_net2 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        self.target_soft_q_net1 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        self.target_soft_q_net2 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "        # Policy\n",
    "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        # Optimizers/Loss\n",
    "        self.soft_q_criterion = nn.MSELoss()\n",
    "        \n",
    "        self.soft_q_optimizer1 = optim.Adam(self.soft_q_net1.parameters(), lr=lr)\n",
    "        self.soft_q_optimizer2 = optim.Adam(self.soft_q_net2.parameters(), lr=lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        \n",
    "        # alpha tuning\n",
    "        self.auto_alpha = auto_alpha\n",
    "        \n",
    "        if self.auto_alpha:\n",
    "            self.target_entropy = -np.prod(env.action_space.shape).item()\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n",
    "            \n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.discount = discount\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        \n",
    "    def get_action(self, state, deterministic=False, explore=False):\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        if explore:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            action  = self.policy_net.get_action(state, deterministic).detach()\n",
    "            return action.numpy()\n",
    "           \n",
    "    def update(self, iterations, batch_size = 100):\n",
    "        \n",
    "        for _ in range(0,iterations):\n",
    "        \n",
    "            state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "            state      = torch.FloatTensor(state).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            action     = torch.FloatTensor(action).to(device)\n",
    "            reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "            done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "            new_actions, policy_mean, policy_log_std, log_pi, *_ = self.policy_net(state)\n",
    "\n",
    "            if self.auto_alpha:\n",
    "                alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "                self.alpha_optimizer.zero_grad()\n",
    "                alpha_loss.backward()\n",
    "                self.alpha_optimizer.step()\n",
    "                alpha = self.log_alpha.exp()\n",
    "            else:\n",
    "                alpha_loss = 0\n",
    "                alpha = 0.2 # constant used by OpenAI\n",
    "\n",
    "            # Update Policy \n",
    "            q_new_actions = torch.min(\n",
    "                self.soft_q_net1(state, new_actions), \n",
    "                self.soft_q_net2(state, new_actions)\n",
    "            )\n",
    "\n",
    "            policy_loss = (alpha*log_pi - q_new_actions).mean()\n",
    "\n",
    "            # Update Soft Q Function\n",
    "            q1_pred = self.soft_q_net1(state, action)\n",
    "            q2_pred = self.soft_q_net2(state, action)\n",
    "\n",
    "            new_next_actions, _, _, new_log_pi, *_ = self.policy_net(next_state)\n",
    "\n",
    "            target_q_values = torch.min(\n",
    "                self.target_soft_q_net1(next_state, new_next_actions),\n",
    "                self.target_soft_q_net2(next_state, new_next_actions),\n",
    "            ) - alpha * new_log_pi\n",
    "\n",
    "            q_target = reward + (1 - done) * self.discount * target_q_values\n",
    "            q1_loss = self.soft_q_criterion(q1_pred, q_target.detach())\n",
    "            q2_loss = self.soft_q_criterion(q2_pred, q_target.detach())\n",
    "\n",
    "            # Update Networks\n",
    "            self.soft_q_optimizer1.zero_grad()\n",
    "            q1_loss.backward()\n",
    "            self.soft_q_optimizer1.step()\n",
    "\n",
    "            self.soft_q_optimizer2.zero_grad()\n",
    "            q2_loss.backward()\n",
    "            self.soft_q_optimizer2.step()\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "            # Soft Updates\n",
    "            for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "                )\n",
    "\n",
    "            for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "                )\n",
    "                \n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.soft_q_net1.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "\n",
    "    def load(self, filename=\"best_avg\", directory=\"./saves\"):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, steps_per_epoch=1000, epochs=100, start_steps=1000, max_ep_len=200):\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"-SAC-Pendulum\")\n",
    "    \n",
    "    # start tracking time\n",
    "    start_time = time.time()\n",
    "    total_rewards = []\n",
    "    avg_reward = None\n",
    "    \n",
    "    # set initial values\n",
    "    o, r, d, ep_reward, ep_len, ep_num = env.reset(), 0, False, 0, 0, 1\n",
    "    \n",
    "    # track total steps\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    \n",
    "    for t in range(1,total_steps):\n",
    "        \n",
    "        explore = t < start_steps\n",
    "        a = agent.get_action(o, explore=explore)\n",
    "        \n",
    "        # Step the env\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_reward += r\n",
    "        ep_len += 1\n",
    "        \n",
    "        writer.add_scalar(\"reward_step\", r, t)\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time\n",
    "        # horizon (that is, when it's an artificial terminal signal\n",
    "        # that isn't based on the agent's state)\n",
    "        d = False if ep_len == max_ep_len else d\n",
    "\n",
    "        # Store experience to replay buffer\n",
    "        replay_buffer.push(o, a, r, o2, d)\n",
    "\n",
    "        # update observation\n",
    "        o = o2\n",
    "        \n",
    "        if d or (ep_len == max_ep_len):\n",
    "        \n",
    "            # carry out update for each step experienced (episode length)\n",
    "            if not explore:\n",
    "                agent.update(ep_len)\n",
    "            \n",
    "            # log progress\n",
    "            total_rewards.append(ep_reward)\n",
    "            avg_reward = np.mean(total_rewards[-10:])\n",
    "            \n",
    "            writer.add_scalar(\"avg_reward\", avg_reward, t)\n",
    "            writer.add_scalar(\"episode_reward\", ep_reward, t)\n",
    "            \n",
    "            print(\"Steps:{} Episode:{} Reward:{} Avg Reward:{}\".format(t, ep_num, ep_reward, avg_reward))\n",
    "            o, r, d, ep_reward, ep_len = env.reset(), 0, False, 0, 0\n",
    "            ep_num += 1\n",
    "            \n",
    "            if avg_reward > -50:\n",
    "                print(\"saving....\")\n",
    "                break\n",
    "\n",
    "        # End of epoch wrap-up\n",
    "        if t > 0 and t % steps_per_epoch == 0:\n",
    "            epoch = t // steps_per_epoch\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:200 Episode:1 Reward:-1591.4926299267513 Avg Reward:-1591.4926299267513\n",
      "Steps:400 Episode:2 Reward:-1189.5296459117349 Avg Reward:-1390.511137919243\n",
      "Steps:600 Episode:3 Reward:-1694.4801102505212 Avg Reward:-1491.8341286963357\n",
      "Steps:800 Episode:4 Reward:-894.021700306629 Avg Reward:-1342.381021598909\n",
      "Steps:1000 Episode:5 Reward:-1733.5362709403935 Avg Reward:-1420.6120714672059\n",
      "Steps:1200 Episode:6 Reward:-1563.7116197210391 Avg Reward:-1444.4619961761782\n",
      "Steps:1400 Episode:7 Reward:-907.676670552222 Avg Reward:-1367.7783782298989\n",
      "Steps:1600 Episode:8 Reward:-1572.2396900164392 Avg Reward:-1393.3360422032163\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "env = \"Pendulum-v0\"#\"RoboschoolHalfCheetah-v1\"\n",
    "\n",
    "replay_buffer = ReplayBuffer(int(1e6))\n",
    "\n",
    "env = NormalizedActions(gym.make(env))\n",
    "\n",
    "agent = SAC(env, replay_buffer)\n",
    "\n",
    "train(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
