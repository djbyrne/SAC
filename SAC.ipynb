{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.linear3 = nn.Linear(hidden_size[1], 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], \n",
    "                 init_w=3e-3, log_std_min=-20, log_std_max=2, epsilon=1e-6):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        \n",
    "        self.mean_linear = nn.Linear(hidden_size[1], num_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "        self.log_std_linear = nn.Linear(hidden_size[1], num_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "    \n",
    "    def forward(self, state, deterministic=False):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        mean    = self.mean_linear(x)\n",
    "        \n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        log_prob = None\n",
    "        \n",
    "        if deterministic:\n",
    "            action = torch.tanh(mean)\n",
    "        else:\n",
    "            # assumes actions have been normalized to (0,1)\n",
    "            normal = Normal(0, 1)\n",
    "            z = mean + std * normal.sample().requires_grad_()\n",
    "            action = torch.tanh(z)\n",
    "            log_prob = Normal(mean, std).log_prob(z) - torch.log(1 - action * action + self.epsilon)\n",
    "            \n",
    "        return action, mean, log_std, log_prob, std\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action,_,_,_,_ =  self.forward(state, deterministic)\n",
    "        act = action.cpu()[0][0]\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.linear3 = nn.Linear(hidden_size[1], 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        \n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "def normalize_action(action, low, high):\n",
    "    action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "    action = np.clip(action, low, high)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(object):\n",
    "    \n",
    "    def __init__(self, env, replay_buffer, seed=0, hidden_dim=[400,300],\n",
    "        steps_per_epoch=200, epochs=1000, discount=0.99,\n",
    "        tau=1e-2, lr=1e-3, auto_alpha=True, batch_size=100, start_steps=10000,\n",
    "        max_ep_len=200, logger_kwargs=dict(), save_freq=1):\n",
    "        \n",
    "        # Set seeds\n",
    "        self.env = env\n",
    "        self.env.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # env space\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0] \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # init networks\n",
    "        \n",
    "        # Soft Q\n",
    "        self.soft_q_net1 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        self.soft_q_net2 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        self.target_soft_q_net1 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        self.target_soft_q_net2 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "        # Policy\n",
    "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        # Optimizers/Loss\n",
    "        self.soft_q_criterion = nn.MSELoss()\n",
    "        \n",
    "        self.soft_q_optimizer1 = optim.Adam(self.soft_q_net1.parameters(), lr=lr)\n",
    "        self.soft_q_optimizer2 = optim.Adam(self.soft_q_net2.parameters(), lr=lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        \n",
    "        # alpha tuning\n",
    "        self.auto_alpha = auto_alpha\n",
    "        \n",
    "        if self.auto_alpha:\n",
    "            self.target_entropy = -np.prod(env.action_space.shape).item()\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n",
    "            \n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.discount = discount\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        \n",
    "    def get_action(self, state, deterministic=False, explore=False):\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        if explore:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            action  = self.policy_net.get_action(state, deterministic).detach()\n",
    "            return action.numpy()\n",
    "           \n",
    "    def update(self, iterations, batch_size = 100):\n",
    "        \n",
    "        for _ in range(0,iterations):\n",
    "        \n",
    "            state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "            state      = torch.FloatTensor(state).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            action     = torch.FloatTensor(action).to(device)\n",
    "            reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "            done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "            new_actions, policy_mean, policy_log_std, log_pi, *_ = self.policy_net(state)\n",
    "\n",
    "            if self.auto_alpha:\n",
    "                alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "                self.alpha_optimizer.zero_grad()\n",
    "                alpha_loss.backward()\n",
    "                self.alpha_optimizer.step()\n",
    "                alpha = self.log_alpha.exp()\n",
    "            else:\n",
    "                alpha_loss = 0\n",
    "                alpha = 0.2 # constant used by OpenAI\n",
    "\n",
    "            # Update Policy \n",
    "            q_new_actions = torch.min(\n",
    "                self.soft_q_net1(state, new_actions), \n",
    "                self.soft_q_net2(state, new_actions)\n",
    "            )\n",
    "\n",
    "            policy_loss = (alpha*log_pi - q_new_actions).mean()\n",
    "\n",
    "            # Update Soft Q Function\n",
    "            q1_pred = self.soft_q_net1(state, action)\n",
    "            q2_pred = self.soft_q_net2(state, action)\n",
    "\n",
    "            new_next_actions, _, _, new_log_pi, *_ = self.policy_net(next_state)\n",
    "\n",
    "            target_q_values = torch.min(\n",
    "                self.target_soft_q_net1(next_state, new_next_actions),\n",
    "                self.target_soft_q_net2(next_state, new_next_actions),\n",
    "            ) - alpha * new_log_pi\n",
    "\n",
    "            q_target = reward + (1 - done) * self.discount * target_q_values\n",
    "            q1_loss = self.soft_q_criterion(q1_pred, q_target.detach())\n",
    "            q2_loss = self.soft_q_criterion(q2_pred, q_target.detach())\n",
    "\n",
    "            # Update Networks\n",
    "            self.soft_q_optimizer1.zero_grad()\n",
    "            q1_loss.backward()\n",
    "            self.soft_q_optimizer1.step()\n",
    "\n",
    "            self.soft_q_optimizer2.zero_grad()\n",
    "            q2_loss.backward()\n",
    "            self.soft_q_optimizer2.step()\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "            # Soft Updates\n",
    "            for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "                )\n",
    "\n",
    "            for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "                )\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, steps_per_epoch=1000, epochs=100, start_steps=1000, max_ep_len=200):\n",
    "    \n",
    "    # start tracking time\n",
    "    start_time = time.time()\n",
    "    total_rewards = []\n",
    "    avg_reward = None\n",
    "    \n",
    "    # set initial values\n",
    "    o, r, d, ep_reward, ep_len, ep_num = env.reset(), 0, False, 0, 0, 1\n",
    "    \n",
    "    # track total steps\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    \n",
    "    for t in range(1,total_steps):\n",
    "        \n",
    "        explore = t < start_steps\n",
    "        a = agent.get_action(o, explore=explore)\n",
    "        \n",
    "        # Step the env\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_reward += r\n",
    "        ep_len += 1\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time\n",
    "        # horizon (that is, when it's an artificial terminal signal\n",
    "        # that isn't based on the agent's state)\n",
    "        d = False if ep_len == max_ep_len else d\n",
    "\n",
    "        # Store experience to replay buffer\n",
    "        replay_buffer.push(o, a, r, o2, d)\n",
    "\n",
    "        # Super critical, easy to overlook step: make sure to update\n",
    "        # most recent observation!\n",
    "        o = o2\n",
    "        \n",
    "        if d or (ep_len == max_ep_len):\n",
    "        \n",
    "            # carry out update for each step experienced (episode length)\n",
    "            if not explore:\n",
    "                agent.update(ep_len)\n",
    "            \n",
    "            # log progress\n",
    "            total_rewards.append(ep_reward)\n",
    "            avg_reward = np.mean(total_rewards[-100:])\n",
    "            \n",
    "            print(\"Steps:{} Episode:{} Reward:{} Avg Reward:{}\".format(t, ep_num, ep_reward, avg_reward))\n",
    "#             logger.store(LossPi=outs[0], LossQ1=outs[1], LossQ2=outs[2],\n",
    "#                          LossV=outs[3], Q1Vals=outs[4], Q2Vals=outs[5],\n",
    "#                          VVals=outs[6], LogPi=outs[7])\n",
    "\n",
    "#             logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "            o, r, d, ep_reward, ep_len = env.reset(), 0, False, 0, 0\n",
    "            ep_num += 1\n",
    "\n",
    "        # End of epoch wrap-up\n",
    "        if t > 0 and t % steps_per_epoch == 0:\n",
    "            epoch = t // steps_per_epoch\n",
    "\n",
    "            # TODO: Save Model\n",
    "\n",
    "            # TODO: Test\n",
    "\n",
    "            # TODO: Log Epoch Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:200 Episode:1 Reward:-1591.2489940105897 Avg Reward:-1591.2489940105897\n",
      "Steps:400 Episode:2 Reward:-1078.2943458073535 Avg Reward:-1334.7716699089715\n",
      "Steps:600 Episode:3 Reward:-1618.782222872674 Avg Reward:-1429.4418542302058\n",
      "Steps:800 Episode:4 Reward:-906.2604868335143 Avg Reward:-1298.646512381033\n",
      "Steps:1000 Episode:5 Reward:-1559.5143962219684 Avg Reward:-1350.8200891492202\n",
      "Steps:1200 Episode:6 Reward:-1564.0601108917206 Avg Reward:-1386.3600927729701\n",
      "Steps:1400 Episode:7 Reward:-1003.9059879461525 Avg Reward:-1331.7237920834248\n",
      "Steps:1600 Episode:8 Reward:-1594.5723409826269 Avg Reward:-1364.5798606958251\n",
      "Steps:1800 Episode:9 Reward:-1066.0288631119065 Avg Reward:-1331.4075276309454\n",
      "Steps:2000 Episode:10 Reward:-1171.6750906090388 Avg Reward:-1315.4342839287547\n",
      "Steps:2200 Episode:11 Reward:-895.5131095053416 Avg Reward:-1277.2596317084444\n",
      "Steps:2400 Episode:12 Reward:-639.1784421759846 Avg Reward:-1224.086199247406\n",
      "Steps:2600 Episode:13 Reward:-1333.9352426810751 Avg Reward:-1232.5361256653805\n",
      "Steps:2800 Episode:14 Reward:-127.64761512994478 Avg Reward:-1153.6155177699923\n",
      "Steps:3000 Episode:15 Reward:-3.1739398420619422 Avg Reward:-1076.919412574797\n",
      "Steps:3200 Episode:16 Reward:-124.0893775786931 Avg Reward:-1017.3675353875403\n",
      "Steps:3400 Episode:17 Reward:-0.8077379810388436 Avg Reward:-957.5699002459814\n",
      "Steps:3600 Episode:18 Reward:-125.26568245677936 Avg Reward:-911.3307770354701\n",
      "Steps:3800 Episode:19 Reward:-244.54611012525737 Avg Reward:-876.2368471980906\n",
      "Steps:4000 Episode:20 Reward:-1.7028284150905977 Avg Reward:-832.5101462589406\n",
      "Steps:4200 Episode:21 Reward:-127.32092595541914 Avg Reward:-798.9297071968682\n",
      "Steps:4400 Episode:22 Reward:-357.76792953698987 Avg Reward:-778.8768991214191\n",
      "Steps:4600 Episode:23 Reward:-336.3875516926848 Avg Reward:-759.638231841909\n",
      "Steps:4800 Episode:24 Reward:-231.80173035119208 Avg Reward:-737.6450442797958\n",
      "Steps:5000 Episode:25 Reward:-126.99870517867497 Avg Reward:-713.2191907157509\n",
      "Steps:5200 Episode:26 Reward:-228.6623772064236 Avg Reward:-694.5823901961613\n",
      "Steps:5400 Episode:27 Reward:-240.65571696887113 Avg Reward:-677.7702911877432\n",
      "Steps:5600 Episode:28 Reward:-119.7667028940377 Avg Reward:-657.8415916058251\n",
      "Steps:5800 Episode:29 Reward:-120.71338797631036 Avg Reward:-639.3199294117039\n",
      "Steps:6000 Episode:30 Reward:-343.89357417155423 Avg Reward:-629.4723842370323\n",
      "Steps:6200 Episode:31 Reward:-245.46112933648882 Avg Reward:-617.0849244015309\n",
      "Steps:6400 Episode:32 Reward:-239.4204004060894 Avg Reward:-605.2829080266733\n",
      "Steps:6600 Episode:33 Reward:-2.7077281639764834 Avg Reward:-587.02305409144\n",
      "Steps:6800 Episode:34 Reward:-119.39144826656911 Avg Reward:-573.269183331885\n",
      "Steps:7000 Episode:35 Reward:-231.81507422924102 Avg Reward:-563.513351643238\n",
      "Steps:7200 Episode:36 Reward:-244.19949403532274 Avg Reward:-554.6435222652403\n",
      "Steps:7400 Episode:37 Reward:-360.43638093982804 Avg Reward:-549.3946806077968\n",
      "Steps:7600 Episode:38 Reward:-4.380313463124818 Avg Reward:-535.0521972618843\n",
      "Steps:7800 Episode:39 Reward:-122.85598884783518 Avg Reward:-524.4830637128061\n",
      "Steps:8000 Episode:40 Reward:-116.44966461352519 Avg Reward:-514.2822287353242\n",
      "Steps:8200 Episode:41 Reward:-258.53069452235275 Avg Reward:-508.04438643744686\n",
      "Steps:8400 Episode:42 Reward:-355.59214797339905 Avg Reward:-504.4145712359219\n",
      "Steps:8600 Episode:43 Reward:-125.15430387868918 Avg Reward:-495.5945650183118\n",
      "Steps:8800 Episode:44 Reward:-1.3359213855652985 Avg Reward:-484.3614140266585\n",
      "Steps:9000 Episode:45 Reward:-125.049151168842 Avg Reward:-476.37669707426255\n",
      "Steps:9200 Episode:46 Reward:-120.19975786156154 Avg Reward:-468.63372013485605\n",
      "Steps:9400 Episode:47 Reward:-116.13346799422003 Avg Reward:-461.13371477016165\n",
      "Steps:9600 Episode:48 Reward:-120.88041805506973 Avg Reward:-454.0451044219306\n",
      "Steps:9800 Episode:49 Reward:-2.160127457521304 Avg Reward:-444.8229620349019\n",
      "Steps:10000 Episode:50 Reward:-1.245679613576474 Avg Reward:-435.9514163864754\n",
      "Steps:10200 Episode:51 Reward:-231.47184056903333 Avg Reward:-431.9420129390746\n",
      "Steps:10400 Episode:52 Reward:-123.43475908987647 Avg Reward:-426.0091811342823\n",
      "Steps:10600 Episode:53 Reward:-115.9683185321308 Avg Reward:-420.15935353801524\n",
      "Steps:10800 Episode:54 Reward:-119.25633751405623 Avg Reward:-414.5870754634975\n",
      "Steps:11000 Episode:55 Reward:-121.3578289991317 Avg Reward:-409.2556346186908\n",
      "Steps:11200 Episode:56 Reward:-234.82042947833702 Avg Reward:-406.1407202411845\n",
      "Steps:11400 Episode:57 Reward:-127.33282603117007 Avg Reward:-401.2493536760965\n",
      "Steps:11600 Episode:58 Reward:-124.41870732648054 Avg Reward:-396.4764114976549\n",
      "Steps:11800 Episode:59 Reward:-118.61509190086538 Avg Reward:-391.7668976061839\n",
      "Steps:12000 Episode:60 Reward:-115.19619984663954 Avg Reward:-387.1573859768581\n",
      "Steps:12200 Episode:61 Reward:-124.82050094341463 Avg Reward:-382.8567813041787\n",
      "Steps:12400 Episode:62 Reward:-120.01960284850651 Avg Reward:-378.61747197424853\n",
      "Steps:12600 Episode:63 Reward:-114.26610289223866 Avg Reward:-374.4214184967563\n",
      "Steps:12800 Episode:64 Reward:-118.25647811445229 Avg Reward:-370.4188413032828\n",
      "Steps:13000 Episode:65 Reward:-114.64510531991375 Avg Reward:-366.4838607496925\n",
      "Steps:13200 Episode:66 Reward:-122.03210128820248 Avg Reward:-362.78004621239717\n",
      "Steps:13400 Episode:67 Reward:-1.3229855292968362 Avg Reward:-357.3851647096643\n",
      "Steps:13600 Episode:68 Reward:-238.9891936348021 Avg Reward:-355.6440474879752\n",
      "Steps:13800 Episode:69 Reward:-228.39923035309505 Avg Reward:-353.7999197034117\n",
      "Steps:14000 Episode:70 Reward:-116.80920700704621 Avg Reward:-350.4143380934636\n",
      "Steps:14200 Episode:71 Reward:-120.72084582435974 Avg Reward:-347.1792184840396\n",
      "Steps:14400 Episode:72 Reward:-127.35472159175796 Avg Reward:-344.12610047164685\n",
      "Steps:14600 Episode:73 Reward:-225.99648037555025 Avg Reward:-342.5078864977277\n",
      "Steps:14800 Episode:74 Reward:-2.0935715993905637 Avg Reward:-337.90769305315564\n",
      "Steps:15000 Episode:75 Reward:-114.51121445638654 Avg Reward:-334.929073338532\n",
      "Steps:15200 Episode:76 Reward:-2.4535377306375925 Avg Reward:-330.5543952384282\n",
      "Steps:15400 Episode:77 Reward:-115.51726698309426 Avg Reward:-327.76170526108615\n",
      "Steps:15600 Episode:78 Reward:-123.29598403783083 Avg Reward:-325.14034986078804\n",
      "Steps:15800 Episode:79 Reward:-117.39729802054394 Avg Reward:-322.5106909767343\n",
      "Steps:16000 Episode:80 Reward:-128.12260389793911 Avg Reward:-320.0808398882494\n",
      "Steps:16200 Episode:81 Reward:-261.26031134197143 Avg Reward:-319.3546605234805\n",
      "Steps:16400 Episode:82 Reward:-2.5596625334518586 Avg Reward:-315.49130688945576\n",
      "Steps:16600 Episode:83 Reward:-345.7757150945769 Avg Reward:-315.85617927746927\n",
      "Steps:16800 Episode:84 Reward:-131.59860296876255 Avg Reward:-313.6626367023656\n",
      "Steps:17000 Episode:85 Reward:-122.84584178659698 Avg Reward:-311.41773323276834\n",
      "Steps:17200 Episode:86 Reward:-123.86732858281849 Avg Reward:-309.236914574048\n",
      "Steps:17400 Episode:87 Reward:-349.2732824527597 Avg Reward:-309.69710271058494\n",
      "Steps:17600 Episode:88 Reward:-117.17845843028773 Avg Reward:-307.50939084376336\n",
      "Steps:17800 Episode:89 Reward:-122.2972131211845 Avg Reward:-305.42835513901525\n",
      "Steps:18000 Episode:90 Reward:-117.94471073485329 Avg Reward:-303.34520353452456\n",
      "Steps:18200 Episode:91 Reward:-125.26403229376817 Avg Reward:-301.388267586824\n",
      "Steps:18400 Episode:92 Reward:-119.94616825209903 Avg Reward:-299.4160708549248\n",
      "Steps:18600 Episode:93 Reward:-115.85221431904596 Avg Reward:-297.44226594593687\n",
      "Steps:18800 Episode:94 Reward:-124.92009125855927 Avg Reward:-295.6069236620286\n",
      "Steps:19000 Episode:95 Reward:-0.8438638594060408 Avg Reward:-292.50415461147463\n",
      "Steps:19200 Episode:96 Reward:-246.6965955871287 Avg Reward:-292.02699253830434\n",
      "Steps:19400 Episode:97 Reward:-123.19880948190968 Avg Reward:-290.2864958057642\n",
      "Steps:19600 Episode:98 Reward:-117.97115849628786 Avg Reward:-288.52817603730017\n",
      "Steps:19800 Episode:99 Reward:-120.3301527318346 Avg Reward:-286.82920610492175\n",
      "Steps:20000 Episode:100 Reward:-229.49466489777004 Avg Reward:-286.2558606928502\n",
      "Steps:20200 Episode:101 Reward:-119.54477923348229 Avg Reward:-271.53881854507915\n",
      "Steps:20400 Episode:102 Reward:-115.92698393899558 Avg Reward:-261.9151449263956\n",
      "Steps:20600 Episode:103 Reward:-241.69843398474237 Avg Reward:-248.14430703751626\n",
      "Steps:20800 Episode:104 Reward:-240.9101590170717 Avg Reward:-241.49080375935185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:21000 Episode:105 Reward:-239.99663472734778 Avg Reward:-228.29562614440562\n",
      "Steps:21200 Episode:106 Reward:-120.56298173244635 Avg Reward:-213.86065485281287\n",
      "Steps:21400 Episode:107 Reward:-119.83166506931998 Avg Reward:-205.01991162404454\n",
      "Steps:21600 Episode:108 Reward:-229.8845654744357 Avg Reward:-191.37303386896264\n",
      "Steps:21800 Episode:109 Reward:-120.61884414436409 Avg Reward:-181.9189336792872\n",
      "Steps:22000 Episode:110 Reward:-240.50445955445988 Avg Reward:-172.6072273687414\n",
      "Steps:22200 Episode:111 Reward:-127.1494582789575 Avg Reward:-164.9235908564776\n",
      "Steps:22400 Episode:112 Reward:-130.33207458735328 Avg Reward:-159.8351271805913\n",
      "Steps:22600 Episode:113 Reward:-228.2834473311704 Avg Reward:-148.77860922709226\n",
      "Steps:22800 Episode:114 Reward:-118.37613525742181 Avg Reward:-148.68589442836702\n",
      "Steps:23000 Episode:115 Reward:-117.18248496089025 Avg Reward:-149.8259798795553\n",
      "Steps:23200 Episode:116 Reward:-223.96628127501128 Avg Reward:-150.82474891651847\n",
      "Steps:23400 Episode:117 Reward:-243.46913372951744 Avg Reward:-153.25136287400326\n",
      "Steps:23600 Episode:118 Reward:-127.05901454868807 Avg Reward:-153.26929619492236\n",
      "Steps:23800 Episode:119 Reward:-117.62764026098758 Avg Reward:-152.00011149627963\n",
      "Steps:24000 Episode:120 Reward:-122.99606379201268 Avg Reward:-153.21304385004885\n",
      "Steps:24200 Episode:121 Reward:-240.1229447018438 Avg Reward:-154.3410640375131\n",
      "Steps:24400 Episode:122 Reward:-119.1452576793279 Avg Reward:-151.95483731893646\n",
      "Steps:24600 Episode:123 Reward:-119.36796381008045 Avg Reward:-149.78464144011042\n",
      "Steps:24800 Episode:124 Reward:-242.28021735577803 Avg Reward:-149.88942631015627\n",
      "Steps:25000 Episode:125 Reward:-126.5298622209748 Avg Reward:-149.88473788057928\n",
      "Steps:25200 Episode:126 Reward:-246.8527496535272 Avg Reward:-150.06664160505036\n",
      "Steps:25400 Episode:127 Reward:-124.26327443363544 Avg Reward:-148.902717179698\n",
      "Steps:25600 Episode:128 Reward:-125.2867701825645 Avg Reward:-148.95791785258328\n",
      "Steps:25800 Episode:129 Reward:-122.9961873665288 Avg Reward:-148.98074584648543\n",
      "Steps:26000 Episode:130 Reward:-222.094332019696 Avg Reward:-147.76275342496686\n",
      "Steps:26200 Episode:131 Reward:-2.508981395431352 Avg Reward:-145.33323194555626\n",
      "Steps:26400 Episode:132 Reward:-241.56499122303032 Avg Reward:-145.3546778537257\n",
      "Steps:26600 Episode:133 Reward:-130.08947782721629 Avg Reward:-146.6284953503581\n",
      "Steps:26800 Episode:134 Reward:-118.32581420672528 Avg Reward:-146.61783900975965\n",
      "Steps:27000 Episode:135 Reward:-244.50433457827506 Avg Reward:-146.74473161325\n",
      "Steps:27200 Episode:136 Reward:-251.93153471864446 Avg Reward:-146.8220520200832\n",
      "Steps:27400 Episode:137 Reward:-116.3277862843929 Avg Reward:-144.38096607352884\n",
      "Steps:27600 Episode:138 Reward:-232.20081849276553 Avg Reward:-146.65917112382525\n",
      "Steps:27800 Episode:139 Reward:-122.15224155941758 Avg Reward:-146.65213365094107\n",
      "Steps:28000 Episode:140 Reward:-234.80652981093291 Avg Reward:-147.83570230291514\n",
      "Steps:28200 Episode:141 Reward:-118.01284959637941 Avg Reward:-146.43052385365542\n",
      "Steps:28400 Episode:142 Reward:-225.82713210311505 Avg Reward:-145.13287369495256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-6e41d9531a61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-238-6fc0e788fb54>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, steps_per_epoch, epochs, start_steps, max_ep_len)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# carry out update for each step experienced (episode length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexplore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# log progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-237-13ab274961fb>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iterations, batch_size)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mq2_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_q_net2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mnew_next_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_log_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             target_q_values = torch.min(\n",
      "\u001b[0;32m~/anaconda3/envs/rl_dev/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-198-285c56ecf0e5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state, deterministic)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnormal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(int(1e6))\n",
    "\n",
    "env = NormalizedActions(gym.make(\"Pendulum-v0\"))\n",
    "\n",
    "agent = SAC(env, replay_buffer)\n",
    "\n",
    "train(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
