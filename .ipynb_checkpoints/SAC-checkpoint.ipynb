{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from tensorboardX import SummaryWriter\n",
    "import roboschool\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.linear3 = nn.Linear(hidden_size[1], 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], \n",
    "                 init_w=3e-3, log_std_min=-20, log_std_max=2, epsilon=1e-6):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        \n",
    "        self.mean_linear = nn.Linear(hidden_size[1], num_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "        self.log_std_linear = nn.Linear(hidden_size[1], num_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def rsample(self, return_pretanh_value=False):\n",
    "        \"\"\"\n",
    "        Sampling in the reparameterization case.\n",
    "        \"\"\"\n",
    "        z = (\n",
    "            self.normal_mean +\n",
    "            self.normal_std *\n",
    "            Normal(\n",
    "                ptu.zeros(self.normal_mean.size()),\n",
    "                ptu.ones(self.normal_std.size())\n",
    "            ).sample()\n",
    "        )\n",
    "        z.requires_grad_()\n",
    "\n",
    "        if return_pretanh_value:\n",
    "            return torch.tanh(z), z\n",
    "        else:\n",
    "            return torch.tanh(z)\n",
    "    \n",
    "    def forward(self, state, deterministic=False):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        mean    = self.mean_linear(x)\n",
    "        \n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        log_prob = None\n",
    "        \n",
    "        if deterministic:\n",
    "            action = torch.tanh(mean)\n",
    "        else:\n",
    "            # assumes actions have been normalized to (0,1)\n",
    "            normal = Normal(0, 1)\n",
    "            z = mean + std * normal.sample().requires_grad_()\n",
    "            action = torch.tanh(z)\n",
    "            log_prob = Normal(mean, std).log_prob(z) - torch.log(1 - action * action + self.epsilon)\n",
    "            \n",
    "        return action, mean, log_std, log_prob, std\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action,_,_,_,_ =  self.forward(state, deterministic)\n",
    "        act = action.cpu()[0][0]\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.linear3 = nn.Linear(hidden_size[1], 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        \n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "def normalize_action(action, low, high):\n",
    "    action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "    action = np.clip(action, low, high)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(object):\n",
    "    \n",
    "    def __init__(self, env, replay_buffer, seed=0, hidden_dim=[400,300],\n",
    "        steps_per_epoch=200, epochs=1000, discount=0.99,\n",
    "        tau=1e-2, lr=1e-3, auto_alpha=True, batch_size=100, start_steps=10000,\n",
    "        max_ep_len=200, logger_kwargs=dict(), save_freq=1):\n",
    "        \n",
    "        # Set seeds\n",
    "        self.env = env\n",
    "        self.env.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # env space\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0] \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # init networks\n",
    "        \n",
    "        # Soft Q\n",
    "        self.soft_q_net1 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        self.soft_q_net2 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        self.target_soft_q_net1 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        self.target_soft_q_net2 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "        # Policy\n",
    "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        # Optimizers/Loss\n",
    "        self.soft_q_criterion = nn.MSELoss()\n",
    "        \n",
    "        self.soft_q_optimizer1 = optim.Adam(self.soft_q_net1.parameters(), lr=lr)\n",
    "        self.soft_q_optimizer2 = optim.Adam(self.soft_q_net2.parameters(), lr=lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        \n",
    "        # alpha tuning\n",
    "        self.auto_alpha = auto_alpha\n",
    "        \n",
    "        if self.auto_alpha:\n",
    "            self.target_entropy = -np.prod(env.action_space.shape).item()\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n",
    "            \n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.discount = discount\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        \n",
    "    def get_action(self, state, deterministic=False, explore=False):\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        if explore:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            action  = self.policy_net.get_action(state, deterministic).detach()\n",
    "            return action.numpy()\n",
    "           \n",
    "    def update(self, iterations, batch_size = 100):\n",
    "        \n",
    "        for _ in range(0,iterations):\n",
    "        \n",
    "            state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "            state      = torch.FloatTensor(state).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            action     = torch.FloatTensor(action).to(device)\n",
    "            reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "            done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "            new_actions, policy_mean, policy_log_std, log_pi, *_ = self.policy_net(state)\n",
    "\n",
    "            if self.auto_alpha:\n",
    "                alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "                self.alpha_optimizer.zero_grad()\n",
    "                alpha_loss.backward()\n",
    "                self.alpha_optimizer.step()\n",
    "                alpha = self.log_alpha.exp()\n",
    "            else:\n",
    "                alpha_loss = 0\n",
    "                alpha = 0.2 # constant used by OpenAI\n",
    "\n",
    "            # Update Policy \n",
    "            q_new_actions = torch.min(\n",
    "                self.soft_q_net1(state, new_actions), \n",
    "                self.soft_q_net2(state, new_actions)\n",
    "            )\n",
    "\n",
    "            policy_loss = (alpha*log_pi - q_new_actions).mean()\n",
    "\n",
    "            # Update Soft Q Function\n",
    "            q1_pred = self.soft_q_net1(state, action)\n",
    "            q2_pred = self.soft_q_net2(state, action)\n",
    "\n",
    "            new_next_actions, _, _, new_log_pi, *_ = self.policy_net(next_state)\n",
    "\n",
    "            target_q_values = torch.min(\n",
    "                self.target_soft_q_net1(next_state, new_next_actions),\n",
    "                self.target_soft_q_net2(next_state, new_next_actions),\n",
    "            ) - alpha * new_log_pi\n",
    "\n",
    "            q_target = reward + (1 - done) * self.discount * target_q_values\n",
    "            q1_loss = self.soft_q_criterion(q1_pred, q_target.detach())\n",
    "            q2_loss = self.soft_q_criterion(q2_pred, q_target.detach())\n",
    "\n",
    "            # Update Networks\n",
    "            self.soft_q_optimizer1.zero_grad()\n",
    "            q1_loss.backward()\n",
    "            self.soft_q_optimizer1.step()\n",
    "\n",
    "            self.soft_q_optimizer2.zero_grad()\n",
    "            q2_loss.backward()\n",
    "            self.soft_q_optimizer2.step()\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "            # Soft Updates\n",
    "            for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "                )\n",
    "\n",
    "            for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "                )\n",
    "                \n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "\n",
    "    def load(self, filename=\"best_avg\", directory=\"./saves\"):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, steps_per_epoch=1000, epochs=100, start_steps=1000, max_ep_len=200):\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"-SAC-Pendulum\")\n",
    "    \n",
    "    # start tracking time\n",
    "    start_time = time.time()\n",
    "    total_rewards = []\n",
    "    avg_reward = None\n",
    "    \n",
    "    # set initial values\n",
    "    o, r, d, ep_reward, ep_len, ep_num = env.reset(), 0, False, 0, 0, 1\n",
    "    \n",
    "    # track total steps\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    \n",
    "    for t in range(1,total_steps):\n",
    "        \n",
    "        explore = t < start_steps\n",
    "        a = agent.get_action(o, explore=explore)\n",
    "        \n",
    "        # Step the env\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_reward += r\n",
    "        ep_len += 1\n",
    "        \n",
    "        writer.add_scalar(\"reward_step\", r, t)\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time\n",
    "        # horizon (that is, when it's an artificial terminal signal\n",
    "        # that isn't based on the agent's state)\n",
    "        d = False if ep_len == max_ep_len else d\n",
    "\n",
    "        # Store experience to replay buffer\n",
    "        replay_buffer.push(o, a, r, o2, d)\n",
    "\n",
    "        # update observation\n",
    "        o = o2\n",
    "        \n",
    "        if d or (ep_len == max_ep_len):\n",
    "        \n",
    "            # carry out update for each step experienced (episode length)\n",
    "            if not explore:\n",
    "                agent.update(ep_len)\n",
    "            \n",
    "            # log progress\n",
    "            total_rewards.append(ep_reward)\n",
    "            avg_reward = np.mean(total_rewards[-10:])\n",
    "            \n",
    "            writer.add_scalar(\"avg_reward\", avg_reward, t)\n",
    "            writer.add_scalar(\"episode_reward\", ep_reward, t)\n",
    "            \n",
    "            print(\"Steps:{} Episode:{} Reward:{} Avg Reward:{}\".format(t, ep_num, ep_reward, avg_reward))\n",
    "            o, r, d, ep_reward, ep_len = env.reset(), 0, False, 0, 0\n",
    "            ep_num += 1\n",
    "            \n",
    "            if avg_reward > -100:\n",
    "                print(\"saving....\")\n",
    "                agent.save(\"best_avg\",\"saves\")\n",
    "\n",
    "        # End of epoch wrap-up\n",
    "        if t > 0 and t % steps_per_epoch == 0:\n",
    "            epoch = t // steps_per_epoch\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:200 Episode:1 Reward:-1748.1950637816087 Avg Reward:-1748.1950637816087\n",
      "Steps:400 Episode:2 Reward:-1022.4790374575991 Avg Reward:-1385.3370506196038\n",
      "Steps:600 Episode:3 Reward:-1363.6087808255468 Avg Reward:-1378.0942940215848\n",
      "Steps:800 Episode:4 Reward:-1220.6857412041204 Avg Reward:-1338.7421558172186\n",
      "Steps:1000 Episode:5 Reward:-1552.8794515733505 Avg Reward:-1381.569614968445\n",
      "Steps:1200 Episode:6 Reward:-1574.0646871480626 Avg Reward:-1413.6521269983814\n",
      "Steps:1400 Episode:7 Reward:-1040.7361442826461 Avg Reward:-1360.3784151818477\n",
      "Steps:1600 Episode:8 Reward:-1534.446797219198 Avg Reward:-1382.1369629365165\n",
      "Steps:1800 Episode:9 Reward:-1073.017796439336 Avg Reward:-1347.790388881274\n",
      "Steps:2000 Episode:10 Reward:-1151.233710185095 Avg Reward:-1328.1347210116562\n",
      "Steps:2200 Episode:11 Reward:-984.6135263260715 Avg Reward:-1251.7765672661026\n",
      "Steps:2400 Episode:12 Reward:-756.9668672104939 Avg Reward:-1225.225350241392\n",
      "Steps:2600 Episode:13 Reward:-1292.1940512355027 Avg Reward:-1218.0838772823877\n",
      "Steps:2800 Episode:14 Reward:-266.2753743847883 Avg Reward:-1122.6428406004545\n",
      "Steps:3000 Episode:15 Reward:-3.2258953047657912 Avg Reward:-967.677484973596\n",
      "Steps:3200 Episode:16 Reward:-123.2169216890064 Avg Reward:-822.5927084276906\n",
      "Steps:3400 Episode:17 Reward:-1.322106283472435 Avg Reward:-718.651304627773\n",
      "Steps:3600 Episode:18 Reward:-127.82531732615996 Avg Reward:-577.9891566384692\n",
      "Steps:3800 Episode:19 Reward:-245.25507069472658 Avg Reward:-495.2128840640083\n",
      "Steps:4000 Episode:20 Reward:-0.3829931282162881 Avg Reward:-380.1278123583204\n",
      "Steps:4200 Episode:21 Reward:-126.70785429299453 Avg Reward:-294.3372451550127\n",
      "Steps:4400 Episode:22 Reward:-355.8513275973143 Avg Reward:-254.22569119369467\n",
      "Steps:4600 Episode:23 Reward:-325.72228728756005 Avg Reward:-157.57851479890047\n",
      "Steps:4800 Episode:24 Reward:-232.1830786609921 Avg Reward:-154.16928522652086\n",
      "Steps:5000 Episode:25 Reward:-127.21258754675208 Avg Reward:-166.56795445071947\n",
      "Steps:5200 Episode:26 Reward:-228.53391021024805 Avg Reward:-177.09965330284362\n",
      "Steps:5400 Episode:27 Reward:-243.42336589647894 Avg Reward:-201.3097792641443\n",
      "Steps:5600 Episode:28 Reward:-121.69666220016634 Avg Reward:-200.69691375154494\n",
      "Steps:5800 Episode:29 Reward:-122.28008973709846 Avg Reward:-188.39941565578212\n",
      "Steps:6000 Episode:30 Reward:-342.2284654000618 Avg Reward:-222.58396288296666\n",
      "Steps:6200 Episode:31 Reward:-245.31274781087174 Avg Reward:-234.4444522347544\n",
      "Steps:6400 Episode:32 Reward:-241.23691429117048 Avg Reward:-222.98301090413997\n",
      "Steps:6600 Episode:33 Reward:-2.0798205546386224 Avg Reward:-190.61876423084786\n",
      "Steps:6800 Episode:34 Reward:-119.73428148633124 Avg Reward:-179.3738845133818\n",
      "Steps:7000 Episode:35 Reward:-231.08030624996044 Avg Reward:-189.76065638370258\n",
      "Steps:7200 Episode:36 Reward:-244.22048527488604 Avg Reward:-191.3293138901664\n",
      "Steps:7400 Episode:37 Reward:-236.45069257903637 Avg Reward:-190.63204655842213\n",
      "Steps:7600 Episode:38 Reward:-5.1435390487217365 Avg Reward:-178.9767342432777\n",
      "Steps:7800 Episode:39 Reward:-123.565364367827 Avg Reward:-179.10526170635052\n",
      "Steps:8000 Episode:40 Reward:-117.33573650614683 Avg Reward:-156.61598881695903\n",
      "Steps:8200 Episode:41 Reward:-255.84958518983765 Avg Reward:-157.66967255485562\n",
      "Steps:8400 Episode:42 Reward:-350.83063275817415 Avg Reward:-168.62904440155603\n",
      "Steps:8600 Episode:43 Reward:-124.07695972042059 Avg Reward:-180.8287583181342\n",
      "Steps:8800 Episode:44 Reward:-0.9690670722929271 Avg Reward:-168.95223687673038\n",
      "Steps:9000 Episode:45 Reward:-126.52142100901871 Avg Reward:-158.4963483526362\n",
      "Steps:9200 Episode:46 Reward:-120.60996454197597 Avg Reward:-146.1352962793452\n",
      "Steps:9400 Episode:47 Reward:-116.05341783088647 Avg Reward:-134.09556880453022\n",
      "Steps:9600 Episode:48 Reward:-119.54559262612216 Avg Reward:-145.53577416227026\n",
      "Steps:9800 Episode:49 Reward:-2.3136449927260614 Avg Reward:-133.41060222476014\n",
      "Steps:10000 Episode:50 Reward:-1.1751369194137453 Avg Reward:-121.79454226608684\n",
      "Steps:10200 Episode:51 Reward:-231.91714872157513 Avg Reward:-119.40129861926059\n",
      "Steps:10400 Episode:52 Reward:-122.62410872939712 Avg Reward:-96.58064621638289\n",
      "saving....\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SAC' object has no attribute 'actor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-7fe0f2965a0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-b99104f14711>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, steps_per_epoch, epochs, start_steps, max_ep_len)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mavg_reward\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saving....\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_avg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"saves\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# End of epoch wrap-up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-7166c86545b4>\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, directory)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%s/%s_actor.pth'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%s/%s_critic.pth'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SAC' object has no attribute 'actor'"
     ]
    }
   ],
   "source": [
    "env = \"Pendulum-v0\"#\"RoboschoolHalfCheetah-v1\"\n",
    "\n",
    "replay_buffer = ReplayBuffer(int(1e6))\n",
    "\n",
    "env = NormalizedActions(gym.make(env))\n",
    "\n",
    "agent = SAC(env, replay_buffer)\n",
    "\n",
    "train(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
