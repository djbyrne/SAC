{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.linear3 = nn.Linear(hidden_size[1], 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], \n",
    "                 init_w=3e-3, log_std_min=-20, log_std_max=2, epsilon=1e-6):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        \n",
    "        self.mean_linear = nn.Linear(hidden_size[1], num_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "        self.log_std_linear = nn.Linear(hidden_size[1], num_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "    \n",
    "    def forward(self, state, deterministic=False):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        mean    = self.mean_linear(x)\n",
    "        \n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        log_prob = None\n",
    "        \n",
    "        if deterministic:\n",
    "            action = torch.tanh(mean)\n",
    "        else:\n",
    "            # assumes actions have been normalized to (0,1)\n",
    "            normal = Normal(0, 1)\n",
    "            z = mean + std * normal.sample().requires_grad_()\n",
    "            action = torch.tanh(z)\n",
    "            log_prob = Normal(mean, std).log_prob(z) - torch.log(1 - action * action + self.epsilon)\n",
    "            \n",
    "        return action, mean, log_std, log_prob, std\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action,_,_,_,_ =  self.forward(state, deterministic)\n",
    "        act = action.cpu()[0][0]\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=[400,300], init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size[0])\n",
    "        self.linear2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.linear3 = nn.Linear(hidden_size[1], 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        \n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "#         print(type(action))\n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "def normalize_action(action, low, high):\n",
    "    action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "    action = np.clip(action, low, high)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(object):\n",
    "    \n",
    "    def __init__(self, env, replay_buffer, seed=0, hidden_dim=[400,300],\n",
    "        steps_per_epoch=200, epochs=1000, discount=0.99,\n",
    "        tau=1e-2, lr=1e-3, auto_alpha=True, batch_size=100, start_steps=10000,\n",
    "        max_ep_len=200, logger_kwargs=dict(), save_freq=1):\n",
    "        \n",
    "        # Set seeds\n",
    "        self.env = env\n",
    "        self.env.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # env space\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0] \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # init networks\n",
    "        \n",
    "        # Soft Q\n",
    "        self.soft_q_net1 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        self.soft_q_net2 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        self.target_soft_q_net1 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        self.target_soft_q_net2 = SoftQNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "        # Policy\n",
    "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim, self.hidden_dim).to(device)\n",
    "        \n",
    "        # Optimizers/Loss\n",
    "        self.soft_q_criterion = nn.MSELoss()\n",
    "        \n",
    "        self.soft_q_optimizer1 = optim.Adam(self.soft_q_net1.parameters(), lr=lr)\n",
    "        self.soft_q_optimizer2 = optim.Adam(self.soft_q_net2.parameters(), lr=lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        \n",
    "        # alpha tuning\n",
    "        self.auto_alpha = auto_alpha\n",
    "        \n",
    "        if self.auto_alpha:\n",
    "            self.target_entropy = -np.prod(env.action_space.shape).item()\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n",
    "            \n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.discount = discount\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        \n",
    "    def get_action(self, state, deterministic=False, explore=False):\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        if explore:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            action  = self.policy_net.get_action(state, deterministic).detach()\n",
    "            return action.numpy()\n",
    "           \n",
    "    def update(self, iterations, batch_size = 100):\n",
    "        \n",
    "        for _ in range(0,iterations):\n",
    "        \n",
    "            state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "            state      = torch.FloatTensor(state).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            action     = torch.FloatTensor(action).to(device)\n",
    "            reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "            done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "            new_actions, policy_mean, policy_log_std, log_pi, *_ = self.policy_net(state)\n",
    "\n",
    "            if self.auto_alpha:\n",
    "                alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "                self.alpha_optimizer.zero_grad()\n",
    "                alpha_loss.backward()\n",
    "                self.alpha_optimizer.step()\n",
    "                alpha = self.log_alpha.exp()\n",
    "            else:\n",
    "                alpha_loss = 0\n",
    "                alpha = 0.2 # constant used by OpenAI\n",
    "\n",
    "            # Update Policy \n",
    "            q_new_actions = torch.min(\n",
    "                self.soft_q_net1(state, new_actions), \n",
    "                self.soft_q_net2(state, new_actions)\n",
    "            )\n",
    "\n",
    "            policy_loss = (alpha*log_pi - q_new_actions).mean()\n",
    "\n",
    "            # Update Soft Q Function\n",
    "            q1_pred = self.soft_q_net1(state, action)\n",
    "            q2_pred = self.soft_q_net2(state, action)\n",
    "\n",
    "            new_next_actions, _, _, new_log_pi, *_ = self.policy_net(next_state)\n",
    "\n",
    "            target_q_values = torch.min(\n",
    "                self.target_soft_q_net1(next_state, new_next_actions),\n",
    "                self.target_soft_q_net2(next_state, new_next_actions),\n",
    "            ) - alpha * new_log_pi\n",
    "\n",
    "            q_target = reward + (1 - done) * self.discount * target_q_values\n",
    "            q1_loss = self.soft_q_criterion(q1_pred, q_target.detach())\n",
    "            q2_loss = self.soft_q_criterion(q2_pred, q_target.detach())\n",
    "\n",
    "            # Update Networks\n",
    "            self.soft_q_optimizer1.zero_grad()\n",
    "            q1_loss.backward()\n",
    "            self.soft_q_optimizer1.step()\n",
    "\n",
    "            self.soft_q_optimizer2.zero_grad()\n",
    "            q2_loss.backward()\n",
    "            self.soft_q_optimizer2.step()\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "            # Soft Updates\n",
    "            for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "                )\n",
    "\n",
    "            for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "                )\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, steps_per_epoch=1000, epochs=100, start_steps=1000, max_ep_len=200):\n",
    "    \n",
    "    # start tracking time\n",
    "    start_time = time.time()\n",
    "    total_rewards = []\n",
    "    avg_reward = None\n",
    "    \n",
    "    # set initial values\n",
    "    o, r, d, ep_reward, ep_len, ep_num = env.reset(), 0, False, 0, 0, 1\n",
    "    \n",
    "    # track total steps\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    \n",
    "    for t in range(1,total_steps):\n",
    "        \n",
    "        explore = t < start_steps\n",
    "        a = agent.get_action(o, explore=explore)\n",
    "        \n",
    "        # Step the env\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_reward += r\n",
    "        ep_len += 1\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time\n",
    "        # horizon (that is, when it's an artificial terminal signal\n",
    "        # that isn't based on the agent's state)\n",
    "        d = False if ep_len == max_ep_len else d\n",
    "\n",
    "        # Store experience to replay buffer\n",
    "        replay_buffer.push(o, a, r, o2, d)\n",
    "\n",
    "        # Super critical, easy to overlook step: make sure to update\n",
    "        # most recent observation!\n",
    "        o = o2\n",
    "        \n",
    "        if d or (ep_len == max_ep_len):\n",
    "        \n",
    "            # carry out update for each step experienced (episode length)\n",
    "            if not explore:\n",
    "                agent.update(ep_len)\n",
    "            \n",
    "            # log progress\n",
    "            total_rewards.append(ep_reward)\n",
    "            avg_reward = np.mean(total_rewards[-100:])\n",
    "            \n",
    "            print(\"Steps:{} Episode:{} Reward:{} Avg Reward:{}\".format(t, ep_num, ep_reward, avg_reward))\n",
    "#             logger.store(LossPi=outs[0], LossQ1=outs[1], LossQ2=outs[2],\n",
    "#                          LossV=outs[3], Q1Vals=outs[4], Q2Vals=outs[5],\n",
    "#                          VVals=outs[6], LogPi=outs[7])\n",
    "\n",
    "#             logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "            o, r, d, ep_reward, ep_len = env.reset(), 0, False, 0, 0\n",
    "            ep_num += 1\n",
    "\n",
    "        # End of epoch wrap-up\n",
    "        if t > 0 and t % steps_per_epoch == 0:\n",
    "            epoch = t // steps_per_epoch\n",
    "\n",
    "            # TODO: Save Model\n",
    "\n",
    "            # TODO: Test\n",
    "\n",
    "            # TODO: Log Epoch Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.961369]\n",
      "[-0.91913414]\n",
      "[-0.13342695]\n",
      "[-0.9454389]\n",
      "[-0.62280065]\n",
      "[0.17876793]\n",
      "[1.9131696]\n",
      "[-1.2539531]\n",
      "[-0.13943326]\n",
      "[1.4449296]\n",
      "[1.1402327]\n",
      "[1.9185568]\n",
      "[0.44484454]\n",
      "[1.8396156]\n",
      "[0.4098439]\n",
      "[1.2640238]\n",
      "[-1.8887649]\n",
      "[0.44798413]\n",
      "[-1.5531621]\n",
      "[-1.1778177]\n",
      "[-0.3436272]\n",
      "[-0.41549912]\n",
      "[-1.1123116]\n",
      "[-0.6287385]\n",
      "[-1.2127985]\n",
      "[0.18342733]\n",
      "[-0.4959697]\n",
      "[-0.567581]\n",
      "[-0.15538284]\n",
      "[1.1346236]\n",
      "[0.19731544]\n",
      "[-1.6494789]\n",
      "[-1.1260362]\n",
      "[1.4704205]\n",
      "[-1.5637068]\n",
      "[1.5414039]\n",
      "[-0.19660148]\n",
      "[-0.6757711]\n",
      "[-0.6137474]\n",
      "[0.92653304]\n",
      "[-0.49984825]\n",
      "[0.8910688]\n",
      "[1.9056537]\n",
      "[-0.38053614]\n",
      "[1.929654]\n",
      "[0.66295654]\n",
      "[0.88624376]\n",
      "[-0.44873464]\n",
      "[1.6907885]\n",
      "[-0.07898264]\n",
      "[0.05012786]\n",
      "[-1.3936604]\n",
      "[1.9178231]\n",
      "[1.468941]\n",
      "[0.20437585]\n",
      "[0.18326975]\n",
      "[1.076142]\n",
      "[1.423168]\n",
      "[-0.70745474]\n",
      "[1.7322052]\n",
      "[1.8172251]\n",
      "[-1.254429]\n",
      "[1.776721]\n",
      "[-0.71179664]\n",
      "[0.10614445]\n",
      "[0.74974483]\n",
      "[1.1739401]\n",
      "[1.8562659]\n",
      "[1.621649]\n",
      "[0.09180963]\n",
      "[-0.9058365]\n",
      "[0.6976866]\n",
      "[0.2945945]\n",
      "[-0.11644472]\n",
      "[-0.5864461]\n",
      "[1.6720432]\n",
      "[-1.7139947]\n",
      "[0.6933694]\n",
      "[0.3323298]\n",
      "[1.3568181]\n",
      "[-1.5804569]\n",
      "[0.9682394]\n",
      "[1.5906342]\n",
      "[-1.548763]\n",
      "[1.5072461]\n",
      "[-0.39436886]\n",
      "[1.8648105]\n",
      "[1.7811958]\n",
      "[1.7706264]\n",
      "[-0.23151775]\n",
      "[0.810792]\n",
      "[0.27733538]\n",
      "[-1.836156]\n",
      "[1.0803264]\n",
      "[1.7844901]\n",
      "[-1.3193734]\n",
      "[-1.6496704]\n",
      "[-0.6883474]\n",
      "[-1.8221418]\n",
      "[-1.260651]\n",
      "[0.93541104]\n",
      "[-1.6593167]\n",
      "[-1.7331537]\n",
      "[-1.821839]\n",
      "[-0.25932503]\n",
      "[0.9129978]\n",
      "[-0.8819161]\n",
      "[-0.948096]\n",
      "[0.75841874]\n",
      "[1.8544312]\n",
      "[-0.98388654]\n",
      "[-0.41808075]\n",
      "[-1.678537]\n",
      "[1.3044602]\n",
      "[1.0214106]\n",
      "[1.5613508]\n",
      "[-0.02898775]\n",
      "[0.42740747]\n",
      "[0.05589106]\n",
      "[0.30596095]\n",
      "[-1.6156712]\n",
      "[1.8979189]\n",
      "[-1.6311502]\n",
      "[0.33828884]\n",
      "[0.15970393]\n",
      "[1.7268813]\n",
      "[-1.595916]\n",
      "[0.942293]\n",
      "[-1.9562708]\n",
      "[0.43592882]\n",
      "[-1.1807251]\n",
      "[1.4389024]\n",
      "[-0.7887907]\n",
      "[-1.4749012]\n",
      "[1.8080784]\n",
      "[-0.04214699]\n",
      "[1.1260796]\n",
      "[-1.0982937]\n",
      "[-1.8372867]\n",
      "[-1.5686504]\n",
      "[0.17215388]\n",
      "[-1.8436846]\n",
      "[0.12222791]\n",
      "[-0.12337522]\n",
      "[-0.75605434]\n",
      "[-1.8687254]\n",
      "[1.785152]\n",
      "[0.37355614]\n",
      "[-1.6050234]\n",
      "[0.75807595]\n",
      "[0.3406929]\n",
      "[0.17005901]\n",
      "[-1.695411]\n",
      "[-0.6802706]\n",
      "[-0.76834685]\n",
      "[-0.9685953]\n",
      "[1.0653709]\n",
      "[-1.7616177]\n",
      "[-1.7665231]\n",
      "[1.266705]\n",
      "[0.98391306]\n",
      "[-0.80611485]\n",
      "[-0.486602]\n",
      "[1.245163]\n",
      "[-1.642197]\n",
      "[1.6718785]\n",
      "[0.57260513]\n",
      "[0.31300896]\n",
      "[-1.1312776]\n",
      "[-1.9709966]\n",
      "[-1.7610489]\n",
      "[-1.1030666]\n",
      "[-1.1464453]\n",
      "[1.0165561]\n",
      "[0.29862168]\n",
      "[-0.30107778]\n",
      "[-1.5341333]\n",
      "[0.32287973]\n",
      "[-0.65662587]\n",
      "[-1.7439795]\n",
      "[-1.4263952]\n",
      "[0.07640515]\n",
      "[-1.5178987]\n",
      "[-1.2179794]\n",
      "[1.6987885]\n",
      "[1.1029668]\n",
      "[-0.9408591]\n",
      "[0.10084967]\n",
      "[-1.0910356]\n",
      "[0.54688096]\n",
      "[0.38919482]\n",
      "[-0.02053456]\n",
      "[-0.01310595]\n",
      "[0.4486146]\n",
      "[-0.07894901]\n",
      "[-0.41773185]\n",
      "[1.62562]\n",
      "[1.2619529]\n",
      "[0.6546356]\n",
      "[1.6045403]\n",
      "Steps:200 Episode:1 Reward:-1610.8799806498682 Avg Reward:-1610.8799806498682\n",
      "[-1.3378361]\n",
      "[-1.3995793]\n",
      "[-1.1103146]\n",
      "[-0.97069174]\n",
      "[1.6556929]\n",
      "[0.9013118]\n",
      "[1.011421]\n",
      "[-0.07330543]\n",
      "[-0.90001225]\n",
      "[1.460286]\n",
      "[-0.30382416]\n",
      "[0.9378895]\n",
      "[-0.30842337]\n",
      "[-0.723384]\n",
      "[-1.4331913]\n",
      "[0.01905923]\n",
      "[-0.90667915]\n",
      "[-0.02424205]\n",
      "[1.6548188]\n",
      "[0.02824968]\n",
      "[0.8738118]\n",
      "[-1.3617059]\n",
      "[-0.5590389]\n",
      "[1.8345693]\n",
      "[-1.4528446]\n",
      "[-0.11318266]\n",
      "[1.5494934]\n",
      "[-0.6428742]\n",
      "[0.00100038]\n",
      "[1.523705]\n",
      "[1.3230098]\n",
      "[1.6526539]\n",
      "[-0.56820923]\n",
      "[-1.2486588]\n",
      "[-0.04888465]\n",
      "[-0.17091995]\n",
      "[-1.7497075]\n",
      "[0.03960166]\n",
      "[-0.03750022]\n",
      "[0.29028425]\n",
      "[-0.46256316]\n",
      "[0.50084525]\n",
      "[-0.48904297]\n",
      "[-1.0988804]\n",
      "[-0.0097175]\n",
      "[-0.37580374]\n",
      "[0.6981173]\n",
      "[1.5609716]\n",
      "[0.2960732]\n",
      "[0.98666435]\n",
      "[-0.74370104]\n",
      "[-0.17292815]\n",
      "[-0.4848301]\n",
      "[-0.4056531]\n",
      "[0.8297779]\n",
      "[-1.8637426]\n",
      "[-0.7864117]\n",
      "[0.12183277]\n",
      "[-0.46608987]\n",
      "[1.3781062]\n",
      "[0.9667708]\n",
      "[-1.1294276]\n",
      "[-1.6949216]\n",
      "[-0.0140389]\n",
      "[-1.2284961]\n",
      "[0.41288465]\n",
      "[0.25229314]\n",
      "[0.44082382]\n",
      "[1.3823771]\n",
      "[0.895616]\n",
      "[-0.03677146]\n",
      "[-0.53363687]\n",
      "[0.6799039]\n",
      "[-0.8539269]\n",
      "[-1.315182]\n",
      "[-1.7506624]\n",
      "[-0.44116497]\n",
      "[1.4223813]\n",
      "[0.4386193]\n",
      "[1.1216003]\n",
      "[-0.54867536]\n",
      "[-0.41155615]\n",
      "[1.4711003]\n",
      "[0.07691735]\n",
      "[0.6876774]\n",
      "[1.9079101]\n",
      "[-1.4700152]\n",
      "[1.1657523]\n",
      "[0.89934164]\n",
      "[-0.32069674]\n",
      "[0.14314859]\n",
      "[0.16553567]\n",
      "[-1.0439932]\n",
      "[0.67215276]\n",
      "[0.67949545]\n",
      "[-0.9384029]\n",
      "[1.993542]\n",
      "[-1.6370633]\n",
      "[-1.4355861]\n",
      "[1.3046647]\n",
      "[-0.34419784]\n",
      "[0.7900115]\n",
      "[0.09513424]\n",
      "[-0.92986435]\n",
      "[0.57874984]\n",
      "[1.5384225]\n",
      "[0.55641925]\n",
      "[0.4028177]\n",
      "[0.7256501]\n",
      "[-1.8233922]\n",
      "[1.7154127]\n",
      "[-1.2449248]\n",
      "[0.812473]\n",
      "[-0.94188213]\n",
      "[0.8302842]\n",
      "[1.8078213]\n",
      "[1.2411164]\n",
      "[-0.5077559]\n",
      "[-1.4380295]\n",
      "[-1.8779712]\n",
      "[-0.04984814]\n",
      "[-1.0140265]\n",
      "[-1.1394271]\n",
      "[-0.8448252]\n",
      "[-0.66716677]\n",
      "[-1.8613828]\n",
      "[0.04667894]\n",
      "[1.9216645]\n",
      "[-0.36593714]\n",
      "[-1.5918148]\n",
      "[1.7125511]\n",
      "[-1.9322076]\n",
      "[-1.431306]\n",
      "[-1.4649566]\n",
      "[-0.2520258]\n",
      "[0.39306903]\n",
      "[-0.09273125]\n",
      "[-0.5510352]\n",
      "[-0.02551297]\n",
      "[0.24081269]\n",
      "[0.9622559]\n",
      "[-0.9769332]\n",
      "[0.67213136]\n",
      "[-1.081291]\n",
      "[-0.6675884]\n",
      "[0.56336296]\n",
      "[1.8501376]\n",
      "[-1.508446]\n",
      "[0.03535173]\n",
      "[1.2966235]\n",
      "[-1.338984]\n",
      "[0.28501633]\n",
      "[0.1585126]\n",
      "[0.7000932]\n",
      "[1.0734963]\n",
      "[1.5827999]\n",
      "[-0.8153361]\n",
      "[-1.9785855]\n",
      "[0.00590759]\n",
      "[0.9611492]\n",
      "[-1.4866787]\n",
      "[-1.6884546]\n",
      "[1.14336]\n",
      "[-0.5097249]\n",
      "[-0.57197237]\n",
      "[-1.8665047]\n",
      "[-1.6018013]\n",
      "[1.7067754]\n",
      "[-0.31015193]\n",
      "[-0.18250117]\n",
      "[1.3512594]\n",
      "[-1.5890579]\n",
      "[-0.88324064]\n",
      "[-1.0114591]\n",
      "[0.78682536]\n",
      "[1.3454248]\n",
      "[-0.7986501]\n",
      "[-0.35461232]\n",
      "[-1.5219729]\n",
      "[1.5395861]\n",
      "[0.8662783]\n",
      "[0.4345274]\n",
      "[0.5787686]\n",
      "[-1.5415783]\n",
      "[-0.5412575]\n",
      "[-0.9403858]\n",
      "[0.10355179]\n",
      "[1.6881793]\n",
      "[0.35118243]\n",
      "[0.7339498]\n",
      "[-0.8923357]\n",
      "[0.5066198]\n",
      "[0.95667285]\n",
      "[0.05684816]\n",
      "[-0.51466507]\n",
      "[0.3918373]\n",
      "[-0.1398749]\n",
      "[-0.00606777]\n",
      "[1.4933487]\n",
      "[-0.15369359]\n",
      "Steps:400 Episode:2 Reward:-987.0513354233931 Avg Reward:-1298.9656580366307\n",
      "[-0.14643627]\n",
      "[0.6693533]\n",
      "[-0.2980402]\n",
      "[0.753059]\n",
      "[0.7659695]\n",
      "[1.2449414]\n",
      "[-0.10213255]\n",
      "[-1.7679406]\n",
      "[1.5495691]\n",
      "[0.66913885]\n",
      "[-0.7865601]\n",
      "[0.4992261]\n",
      "[-0.7502299]\n",
      "[0.8958514]\n",
      "[1.4504446]\n",
      "[1.9577882]\n",
      "[1.9572082]\n",
      "[1.385519]\n",
      "[-1.8976156]\n",
      "[1.4989425]\n",
      "[-1.6351824]\n",
      "[1.3010753]\n",
      "[-1.376205]\n",
      "[0.5309063]\n",
      "[-0.18726899]\n",
      "[-1.1802132]\n",
      "[-0.08020965]\n",
      "[0.07540172]\n",
      "[-0.63295764]\n",
      "[-1.3828952]\n",
      "[1.9635662]\n",
      "[-0.28091967]\n",
      "[0.14163198]\n",
      "[0.07243514]\n",
      "[0.5860567]\n",
      "[0.3027353]\n",
      "[1.2319812]\n",
      "[0.02424761]\n",
      "[0.10278051]\n",
      "[1.509064]\n",
      "[-1.7486433]\n",
      "[-1.0499667]\n",
      "[-0.526902]\n",
      "[-1.0663443]\n",
      "[-0.45799884]\n",
      "[0.56951386]\n",
      "[0.44293708]\n",
      "[-1.4568821]\n",
      "[1.0298761]\n",
      "[0.67977256]\n",
      "[-1.3420303]\n",
      "[-0.9367987]\n",
      "[0.9782091]\n",
      "[1.4778668]\n",
      "[-1.2629927]\n",
      "[1.5710608]\n",
      "[-0.4271834]\n",
      "[-0.940654]\n",
      "[1.8282211]\n",
      "[1.5198442]\n",
      "[1.8049612]\n",
      "[-1.0052634]\n",
      "[-1.7948364]\n",
      "[-1.5292037]\n",
      "[-1.573798]\n",
      "[-0.5668489]\n",
      "[-1.726428]\n",
      "[-0.11214449]\n",
      "[1.5374815]\n",
      "[-0.53451383]\n",
      "[-0.26324138]\n",
      "[0.788768]\n",
      "[1.5362604]\n",
      "[-0.7411179]\n",
      "[-0.18380532]\n",
      "[-0.7022698]\n",
      "[0.31447756]\n",
      "[-0.401232]\n",
      "[0.22353692]\n",
      "[-0.19524693]\n",
      "[-1.6945035]\n",
      "[-0.658333]\n",
      "[1.3187704]\n",
      "[1.9301548]\n",
      "[0.3757324]\n",
      "[-1.8912172]\n",
      "[1.0763586]\n",
      "[1.3637332]\n",
      "[1.7376622]\n",
      "[1.0597019]\n",
      "[-0.9285123]\n",
      "[0.8055922]\n",
      "[-1.971702]\n",
      "[-1.099418]\n",
      "[0.6062675]\n",
      "[-1.4406452]\n",
      "[1.6562515]\n",
      "[-1.511224]\n",
      "[0.15673065]\n",
      "[1.2344972]\n",
      "[1.3595589]\n",
      "[-0.16509923]\n",
      "[1.2019325]\n",
      "[-1.058535]\n",
      "[-0.95540494]\n",
      "[-1.6864672]\n",
      "[1.0158101]\n",
      "[-0.38877654]\n",
      "[1.6777562]\n",
      "[0.14743719]\n",
      "[0.47928542]\n",
      "[-0.65117925]\n",
      "[1.7587178]\n",
      "[-0.17474039]\n",
      "[-1.364763]\n",
      "[1.3935242]\n",
      "[0.41415742]\n",
      "[-1.8029602]\n",
      "[-1.5509067]\n",
      "[0.66035163]\n",
      "[1.0834783]\n",
      "[-1.335377]\n",
      "[-1.2956861]\n",
      "[1.284453]\n",
      "[-0.9827434]\n",
      "[-0.00720383]\n",
      "[-1.7347747]\n",
      "[0.27889004]\n",
      "[-0.73627204]\n",
      "[1.6834358]\n",
      "[1.2275907]\n",
      "[-0.7978355]\n",
      "[0.4870565]\n",
      "[-0.39894763]\n",
      "[-1.649569]\n",
      "[-1.747924]\n",
      "[-1.1073576]\n",
      "[0.3638975]\n",
      "[-1.6060737]\n",
      "[-1.2517081]\n",
      "[0.56270707]\n",
      "[1.3481413]\n",
      "[0.07465024]\n",
      "[-1.9341583]\n",
      "[1.3851198]\n",
      "[-0.9166445]\n",
      "[-1.454184]\n",
      "[1.1745872]\n",
      "[-1.8517363]\n",
      "[1.3206087]\n",
      "[1.9501023]\n",
      "[0.22933891]\n",
      "[1.0277246]\n",
      "[1.5883524]\n",
      "[1.5230528]\n",
      "[1.2706763]\n",
      "[-0.16401932]\n",
      "[1.4758635]\n",
      "[0.36396426]\n",
      "[0.42536053]\n",
      "[-0.9981131]\n",
      "[0.58797586]\n",
      "[-1.3989404]\n",
      "[-0.21769983]\n",
      "[-0.26365095]\n",
      "[0.48640448]\n",
      "[-1.0971018]\n",
      "[0.7264784]\n",
      "[1.6718533]\n",
      "[1.6433003]\n",
      "[1.2933965]\n",
      "[0.80379945]\n",
      "[-1.5453879]\n",
      "[-1.4600908]\n",
      "[1.2173452]\n",
      "[-1.9133193]\n",
      "[-1.9870831]\n",
      "[0.76092726]\n",
      "[-1.9229945]\n",
      "[1.7741494]\n",
      "[-0.6442593]\n",
      "[-0.905508]\n",
      "[-1.9728358]\n",
      "[-0.1563697]\n",
      "[-1.6320995]\n",
      "[0.52374333]\n",
      "[-1.1695845]\n",
      "[1.4785382]\n",
      "[1.5181944]\n",
      "[-0.06942829]\n",
      "[1.773465]\n",
      "[-0.58917177]\n",
      "[1.7153274]\n",
      "[0.95585424]\n",
      "[0.55362123]\n",
      "[-1.0565739]\n",
      "[0.13451964]\n",
      "[-1.709744]\n",
      "[-1.0020596]\n",
      "[0.27180108]\n",
      "Steps:600 Episode:3 Reward:-1554.8262032536 Avg Reward:-1384.252506442287\n",
      "[-0.802784]\n",
      "[0.52352387]\n",
      "[1.5314653]\n",
      "[-0.05017998]\n",
      "[-0.00755682]\n",
      "[-1.2430876]\n",
      "[1.6902498]\n",
      "[-1.9314973]\n",
      "[0.8041146]\n",
      "[1.6384343]\n",
      "[1.2732548]\n",
      "[1.7613089]\n",
      "[1.0300883]\n",
      "[1.3403158]\n",
      "[1.994798]\n",
      "[1.3305444]\n",
      "[1.1587049]\n",
      "[-0.7376487]\n",
      "[-1.563385]\n",
      "[-1.7194612]\n",
      "[-1.6970477]\n",
      "[0.63308746]\n",
      "[1.0765102]\n",
      "[-1.2196476]\n",
      "[1.2222459]\n",
      "[1.717494]\n",
      "[-0.28753662]\n",
      "[-1.3597198]\n",
      "[1.7717048]\n",
      "[1.0922608]\n",
      "[1.5788953]\n",
      "[-0.33911413]\n",
      "[1.6333603]\n",
      "[-0.9842322]\n",
      "[-0.3279172]\n",
      "[-1.7618252]\n",
      "[1.4278864]\n",
      "[-0.1937817]\n",
      "[1.0431141]\n",
      "[1.2163008]\n",
      "[-0.80693424]\n",
      "[-1.845879]\n",
      "[-1.679481]\n",
      "[1.4017276]\n",
      "[1.5984409]\n",
      "[1.6406528]\n",
      "[-0.7650373]\n",
      "[-1.4411403]\n",
      "[0.6595947]\n",
      "[1.7109663]\n",
      "[-1.8554026]\n",
      "[-0.52856284]\n",
      "[0.6510394]\n",
      "[0.48587242]\n",
      "[-0.88943493]\n",
      "[1.9976325]\n",
      "[-0.39665434]\n",
      "[-0.16638735]\n",
      "[1.7334481]\n",
      "[0.89540577]\n",
      "[-0.6757109]\n",
      "[0.71779746]\n",
      "[-1.1993426]\n",
      "[-1.7071418]\n",
      "[1.4805324]\n",
      "[-1.3544136]\n",
      "[1.2952935]\n",
      "[-0.56079215]\n",
      "[-0.8087627]\n",
      "[0.02387106]\n",
      "[1.9818006]\n",
      "[-1.3301537]\n",
      "[1.8309494]\n",
      "[-0.96441823]\n",
      "[0.11282785]\n",
      "[0.43231565]\n",
      "[1.8059617]\n",
      "[1.4177107]\n",
      "[1.9001286]\n",
      "[-1.2756968]\n",
      "[1.8204571]\n",
      "[0.54251343]\n",
      "[-0.64709795]\n",
      "[0.41218472]\n",
      "[0.08564481]\n",
      "[0.80387866]\n",
      "[-0.11999742]\n",
      "[0.3478221]\n",
      "[0.57841986]\n",
      "[1.3584851]\n",
      "[-1.4289523]\n",
      "[0.14756937]\n",
      "[0.66699964]\n",
      "[-1.8049465]\n",
      "[-0.5635594]\n",
      "[-0.349924]\n",
      "[0.40608016]\n",
      "[0.901221]\n",
      "[-0.19476603]\n",
      "[0.5756188]\n",
      "[1.9507986]\n",
      "[1.0597078]\n",
      "[0.2568342]\n",
      "[-0.3030291]\n",
      "[-0.87362283]\n",
      "[-1.578981]\n",
      "[-0.35213974]\n",
      "[0.26900324]\n",
      "[1.6484143]\n",
      "[1.505124]\n",
      "[-0.24538848]\n",
      "[-0.687337]\n",
      "[-0.7626244]\n",
      "[-0.4551868]\n",
      "[0.38380662]\n",
      "[-1.5125337]\n",
      "[-0.80445296]\n",
      "[1.2856425]\n",
      "[1.7650058]\n",
      "[1.8225968]\n",
      "[-1.3754028]\n",
      "[-1.3955911]\n",
      "[1.6937473]\n",
      "[-0.9643957]\n",
      "[0.19222133]\n",
      "[-0.2866675]\n",
      "[1.1294795]\n",
      "[0.55774546]\n",
      "[-1.1700919]\n",
      "[0.87979275]\n",
      "[1.5813777]\n",
      "[-0.10781563]\n",
      "[-1.5861853]\n",
      "[-0.05122536]\n",
      "[-0.8443421]\n",
      "[-0.85078245]\n",
      "[-1.8003736]\n",
      "[1.776885]\n",
      "[-1.1000661]\n",
      "[-0.02113734]\n",
      "[0.7866843]\n",
      "[0.6051932]\n",
      "[1.104951]\n",
      "[1.6242216]\n",
      "[1.9104966]\n",
      "[-0.19685403]\n",
      "[0.5854697]\n",
      "[-1.8416202]\n",
      "[-1.324769]\n",
      "[1.2475412]\n",
      "[1.499086]\n",
      "[0.71553737]\n",
      "[-0.22502814]\n",
      "[-0.19248883]\n",
      "[-1.4012073]\n",
      "[0.5022129]\n",
      "[-0.14907464]\n",
      "[0.3163594]\n",
      "[-1.5342864]\n",
      "[-1.0896518]\n",
      "[-0.04147026]\n",
      "[1.7318107]\n",
      "[0.04394436]\n",
      "[-1.9317795]\n",
      "[-1.9678212]\n",
      "[1.3975593]\n",
      "[-0.2598163]\n",
      "[-0.15561277]\n",
      "[0.7559721]\n",
      "[-1.5230861]\n",
      "[-0.0056357]\n",
      "[1.2668808]\n",
      "[1.2585493]\n",
      "[-0.11364222]\n",
      "[0.7887112]\n",
      "[-1.0692016]\n",
      "[-0.06521954]\n",
      "[-0.18427305]\n",
      "[-1.6845145]\n",
      "[0.66971684]\n",
      "[-0.07906936]\n",
      "[-0.6700993]\n",
      "[-0.16808899]\n",
      "[-0.8182897]\n",
      "[-0.8165988]\n",
      "[-1.7751068]\n",
      "[0.17916264]\n",
      "[-0.87451804]\n",
      "[-0.30073056]\n",
      "[-0.46605104]\n",
      "[-0.32029942]\n",
      "[-0.7028644]\n",
      "[0.82378304]\n",
      "[1.1229151]\n",
      "[-1.7330326]\n",
      "[1.9535444]\n",
      "[-0.9909992]\n",
      "[0.861693]\n",
      "[-0.76955533]\n",
      "[-0.8479071]\n",
      "Steps:800 Episode:4 Reward:-1146.7689636001408 Avg Reward:-1324.8816207317504\n",
      "[1.5627086]\n",
      "[-1.4711475]\n",
      "[-1.9197373]\n",
      "[0.3581525]\n",
      "[0.8528831]\n",
      "[-1.3657802]\n",
      "[1.5678269]\n",
      "[1.1798078]\n",
      "[-0.9740304]\n",
      "[0.4322675]\n",
      "[-0.96366286]\n",
      "[-1.0208019]\n",
      "[-1.6803509]\n",
      "[0.7802046]\n",
      "[0.9958934]\n",
      "[-0.929769]\n",
      "[-1.9460645]\n",
      "[-1.4788284]\n",
      "[-1.3497515]\n",
      "[0.3615986]\n",
      "[-1.0889506]\n",
      "[-1.7963953]\n",
      "[-0.43111175]\n",
      "[-1.5312103]\n",
      "[1.5534803]\n",
      "[-0.17440024]\n",
      "[0.57087135]\n",
      "[1.0719424]\n",
      "[-1.1131711]\n",
      "[-1.2081628]\n",
      "[1.0279913]\n",
      "[-1.1700449]\n",
      "[1.2882344]\n",
      "[-1.5431092]\n",
      "[1.383496]\n",
      "[0.153436]\n",
      "[-1.4229038]\n",
      "[-0.6320627]\n",
      "[-0.29024503]\n",
      "[-1.8050573]\n",
      "[0.9574336]\n",
      "[1.5730108]\n",
      "[1.071106]\n",
      "[1.2063001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.868337]\n",
      "[0.92949]\n",
      "[-0.2796036]\n",
      "[0.37579957]\n",
      "[-1.9561723]\n",
      "[-0.9695208]\n",
      "[1.3911844]\n",
      "[0.23366036]\n",
      "[-0.5606659]\n",
      "[0.1912366]\n",
      "[-0.8987101]\n",
      "[-1.0531482]\n",
      "[0.88129723]\n",
      "[1.3854718]\n",
      "[1.247218]\n",
      "[-1.7152478]\n",
      "[1.1119248]\n",
      "[-1.1868219]\n",
      "[0.27178738]\n",
      "[1.9276421]\n",
      "[1.5880587]\n",
      "[-1.8585858]\n",
      "[0.69346225]\n",
      "[-1.1766461]\n",
      "[0.25358725]\n",
      "[-1.6707246]\n",
      "[-0.7288857]\n",
      "[-0.78406805]\n",
      "[-1.1575528]\n",
      "[-1.7311738]\n",
      "[-1.7431059]\n",
      "[1.8038731]\n",
      "[-1.4315345]\n",
      "[1.4881872]\n",
      "[-1.1204625]\n",
      "[-1.591062]\n",
      "[0.94890463]\n",
      "[1.8696387]\n",
      "[0.90945697]\n",
      "[0.56252444]\n",
      "[-0.05278755]\n",
      "[-0.56082684]\n",
      "[-0.41973853]\n",
      "[1.792041]\n",
      "[-1.5432662]\n",
      "[-0.94614774]\n",
      "[1.6426276]\n",
      "[-0.19235218]\n",
      "[-0.4869607]\n",
      "[0.35883823]\n",
      "[0.94910556]\n",
      "[1.2965958]\n",
      "[-1.7566451]\n",
      "[-0.645042]\n",
      "[-1.148022]\n",
      "[0.16450238]\n",
      "[1.1498791]\n",
      "[0.41225064]\n",
      "[-0.7803505]\n",
      "[0.32605657]\n",
      "[1.0362248]\n",
      "[0.5063941]\n",
      "[0.2260614]\n",
      "[-1.1089603]\n",
      "[1.5821264]\n",
      "[1.8504505]\n",
      "[0.13762198]\n",
      "[0.32048628]\n",
      "[1.9745036]\n",
      "[-0.34399512]\n",
      "[0.04952668]\n",
      "[1.1297659]\n",
      "[-0.27623174]\n",
      "[0.9459725]\n",
      "[1.6081138]\n",
      "[-0.801929]\n",
      "[-0.39558068]\n",
      "[-1.032123]\n",
      "[-1.3359414]\n",
      "[-0.22293821]\n",
      "[-1.5494993]\n",
      "[0.9269226]\n",
      "[0.8410728]\n",
      "[-0.15514855]\n",
      "[-0.73422]\n",
      "[-1.9188634]\n",
      "[-0.92562366]\n",
      "[0.87719715]\n",
      "[-1.0490698]\n",
      "[1.4926143]\n",
      "[0.03950828]\n",
      "[0.2654416]\n",
      "[0.84939176]\n",
      "[-1.7927089]\n",
      "[-0.78476644]\n",
      "[1.8268031]\n",
      "[-1.9027381]\n",
      "[0.20593557]\n",
      "[0.55536014]\n",
      "[0.11229901]\n",
      "[-1.7209156]\n",
      "[-1.0809727]\n",
      "[1.5868003]\n",
      "[-0.30826277]\n",
      "[0.9958802]\n",
      "[1.8683463]\n",
      "[-1.3589212]\n",
      "[-0.56912416]\n",
      "[-0.47677168]\n",
      "[-0.70857227]\n",
      "[-1.2569674]\n",
      "[0.5996158]\n",
      "[1.4954871]\n",
      "[-1.9412346]\n",
      "[1.8284248]\n",
      "[-1.3098443]\n",
      "[1.2260693]\n",
      "[-0.5953245]\n",
      "[-0.9616805]\n",
      "[0.13863388]\n",
      "[-1.2370769]\n",
      "[1.81813]\n",
      "[-1.7840433]\n",
      "[-0.17029408]\n",
      "[1.2339876]\n",
      "[-0.9425195]\n",
      "[-0.15634649]\n",
      "[-0.04487797]\n",
      "[-0.7323543]\n",
      "[1.7623615]\n",
      "[-0.40653408]\n",
      "[0.504293]\n",
      "[-0.839339]\n",
      "[-0.8419074]\n",
      "[-1.51788]\n",
      "[-0.21357256]\n",
      "[-1.0555886]\n",
      "[-1.4122955]\n",
      "[-0.18980512]\n",
      "[-1.7433614]\n",
      "[-0.01903919]\n",
      "[-1.885017]\n",
      "[0.4297291]\n",
      "[1.0840973]\n",
      "[-0.8823517]\n",
      "[-0.38629097]\n",
      "[1.2434319]\n",
      "[-1.9362512]\n",
      "[-0.5950113]\n",
      "[-1.5786924]\n",
      "[-1.9468201]\n",
      "[1.3767812]\n",
      "[0.72192055]\n",
      "[-1.6231887]\n",
      "[0.58359003]\n",
      "[-0.911358]\n",
      "Steps:1000 Episode:5 Reward:-1713.4942326814312 Avg Reward:-1402.6041431216865\n",
      "[0.7698536]\n",
      "[-0.01633862]\n",
      "[-0.3527353]\n",
      "[-0.669925]\n",
      "[-0.92039526]\n",
      "[0.5468525]\n",
      "[-0.901156]\n",
      "[-0.04516388]\n",
      "[-0.23366813]\n",
      "[0.522486]\n",
      "[-0.60613096]\n",
      "[0.34013206]\n",
      "[-0.0918454]\n",
      "[-0.24145833]\n",
      "[0.71437734]\n",
      "[0.40965483]\n",
      "[-0.66156673]\n",
      "[0.40530458]\n",
      "[0.06716584]\n",
      "[0.72016406]\n",
      "[-0.81273144]\n",
      "[-0.6161351]\n",
      "[0.33652145]\n",
      "[-0.6330041]\n",
      "[0.24115306]\n",
      "[0.8929974]\n",
      "[0.6822812]\n",
      "[0.12235944]\n",
      "[0.26500723]\n",
      "[-0.18136361]\n",
      "[-0.68152225]\n",
      "[0.69638044]\n",
      "[0.54941386]\n",
      "[0.6953809]\n",
      "[-0.81257117]\n",
      "[0.83343554]\n",
      "[-0.48982948]\n",
      "[-0.22364976]\n",
      "[0.2102954]\n",
      "[0.29587218]\n",
      "[-0.5445433]\n",
      "[0.16415386]\n",
      "[0.27466807]\n",
      "[-0.8667953]\n",
      "[-0.18218619]\n",
      "[-0.9765471]\n",
      "[0.5516196]\n",
      "[-0.09372457]\n",
      "[0.43297938]\n",
      "[0.52359545]\n",
      "[-0.06757623]\n",
      "[0.31269303]\n",
      "[-0.83096164]\n",
      "[0.7746161]\n",
      "[-0.8035227]\n",
      "[-0.4675313]\n",
      "[-0.6441819]\n",
      "[-0.06439103]\n",
      "[-0.12999544]\n",
      "[-0.78746533]\n",
      "[0.02959271]\n",
      "[-0.09004779]\n",
      "[0.22456932]\n",
      "[0.2750451]\n",
      "[0.63266146]\n",
      "[0.06846156]\n",
      "[0.418963]\n",
      "[0.82373327]\n",
      "[0.84991753]\n",
      "[-0.4042834]\n",
      "[0.2442617]\n",
      "[-0.05118745]\n",
      "[-0.7243654]\n",
      "[0.90058535]\n",
      "[-0.9759276]\n",
      "[0.8121935]\n",
      "[-0.8758432]\n",
      "[-0.6304636]\n",
      "[-0.43182287]\n",
      "[0.544567]\n",
      "[-0.69360334]\n",
      "[-0.96862024]\n",
      "[-0.43295908]\n",
      "[-0.33014092]\n",
      "[0.19703338]\n",
      "[0.39161834]\n",
      "[-0.8301712]\n",
      "[-0.08573052]\n",
      "[-0.35962978]\n",
      "[0.71129143]\n",
      "[-0.6226721]\n",
      "[0.42430183]\n",
      "[0.6539994]\n",
      "[0.6447051]\n",
      "[-0.06828362]\n",
      "[0.9785343]\n",
      "[-0.90086347]\n",
      "[-0.22492221]\n",
      "[0.1362006]\n",
      "[0.89076954]\n",
      "[0.24480903]\n",
      "[-0.785722]\n",
      "[0.32007858]\n",
      "[-0.55984724]\n",
      "[-0.6515917]\n",
      "[-0.5350586]\n",
      "[0.32039732]\n",
      "[-0.61602706]\n",
      "[-0.23351091]\n",
      "[0.9872002]\n",
      "[-0.621263]\n",
      "[-0.2171406]\n",
      "[-0.8993239]\n",
      "[-0.53393453]\n",
      "[0.28161678]\n",
      "[-0.43041053]\n",
      "[0.36832747]\n",
      "[0.7611321]\n",
      "[0.37194008]\n",
      "[-0.26649168]\n",
      "[0.5647485]\n",
      "[0.31657937]\n",
      "[-0.9322907]\n",
      "[-0.61027944]\n",
      "[0.34548056]\n",
      "[0.16459888]\n",
      "[0.17276649]\n",
      "[-0.82703745]\n",
      "[-0.91691875]\n",
      "[-0.9238683]\n",
      "[-0.84090763]\n",
      "[0.33669227]\n",
      "[-0.83568597]\n",
      "[0.64765304]\n",
      "[-0.2511738]\n",
      "[0.34651285]\n",
      "[0.3054373]\n",
      "[-0.17005585]\n",
      "[-0.11337753]\n",
      "[0.88155437]\n",
      "[-0.654078]\n",
      "[0.05479909]\n",
      "[-0.5910961]\n",
      "[0.7891093]\n",
      "[-0.02341752]\n",
      "[-0.06618696]\n",
      "[0.5506922]\n",
      "[-0.68201977]\n",
      "[0.6134387]\n",
      "[0.7891267]\n",
      "[-0.06179229]\n",
      "[-0.09585947]\n",
      "[0.39874265]\n",
      "[0.09392621]\n",
      "[-0.79748565]\n",
      "[-0.1973027]\n",
      "[-0.42325628]\n",
      "[0.58987284]\n",
      "[-0.12613548]\n",
      "[-0.8087207]\n",
      "[0.21846405]\n",
      "[0.08981714]\n",
      "[-0.31071696]\n",
      "[0.2846846]\n",
      "[-0.71228737]\n",
      "[-0.44119623]\n",
      "[0.85295725]\n",
      "[0.3655642]\n",
      "[-0.4101417]\n",
      "[0.4898196]\n",
      "[0.3405639]\n",
      "[-0.8167249]\n",
      "[0.81276226]\n",
      "[0.87261236]\n",
      "[0.0427467]\n",
      "[0.88507384]\n",
      "[0.19453645]\n",
      "[-0.7237442]\n",
      "[0.82965106]\n",
      "[-0.19658084]\n",
      "[0.96483594]\n",
      "[0.87174785]\n",
      "[0.47440752]\n",
      "[0.33647576]\n",
      "[0.61952037]\n",
      "[0.5133189]\n",
      "[-0.7835954]\n",
      "[-0.49252176]\n",
      "[-0.25447023]\n",
      "[0.80497926]\n",
      "[-0.40277383]\n",
      "[0.6086393]\n",
      "[-0.5258388]\n",
      "[0.5244741]\n",
      "[-0.91808355]\n",
      "[0.81215346]\n",
      "[0.9558159]\n",
      "[0.2443499]\n",
      "[0.30029625]\n",
      "[0.8409039]\n",
      "Steps:1200 Episode:6 Reward:-1549.1980672015682 Avg Reward:-1427.036463801667\n",
      "[0.97333896]\n",
      "[0.348795]\n",
      "[0.13887984]\n",
      "[-0.8962935]\n",
      "[-0.7011486]\n",
      "[0.71973735]\n",
      "[0.59459096]\n",
      "[0.6763746]\n",
      "[-0.05466382]\n",
      "[-0.8598976]\n",
      "[0.9428506]\n",
      "[-0.6221189]\n",
      "[0.85315]\n",
      "[0.22247046]\n",
      "[0.22919945]\n",
      "[-0.5400094]\n",
      "[0.6991238]\n",
      "[0.34745443]\n",
      "[-0.08797461]\n",
      "[-0.65055954]\n",
      "[-0.6118679]\n",
      "[0.39635813]\n",
      "[0.7401026]\n",
      "[-0.5725669]\n",
      "[0.12651859]\n",
      "[-0.69453967]\n",
      "[0.23026277]\n",
      "[0.52991587]\n",
      "[0.81870204]\n",
      "[-0.954143]\n",
      "[-0.09775295]\n",
      "[-0.2645019]\n",
      "[0.36614645]\n",
      "[0.03898012]\n",
      "[0.8051403]\n",
      "[0.21679097]\n",
      "[-0.15836029]\n",
      "[-0.18507144]\n",
      "[-0.7970403]\n",
      "[-0.03990646]\n",
      "[0.3814513]\n",
      "[-0.43175346]\n",
      "[-0.9013324]\n",
      "[-0.4088464]\n",
      "[-0.21819966]\n",
      "[-0.19178027]\n",
      "[0.6033118]\n",
      "[-0.46575952]\n",
      "[-0.25430122]\n",
      "[0.8162565]\n",
      "[-0.46527386]\n",
      "[0.95532465]\n",
      "[0.5150466]\n",
      "[0.6496036]\n",
      "[0.4269096]\n",
      "[0.7852663]\n",
      "[0.45514145]\n",
      "[-0.99259144]\n",
      "[-0.5848689]\n",
      "[0.7739415]\n",
      "[-0.5855124]\n",
      "[-0.604662]\n",
      "[-0.4811217]\n",
      "[-0.72289217]\n",
      "[0.7637084]\n",
      "[-0.3925684]\n",
      "[0.15668036]\n",
      "[-0.59348685]\n",
      "[-0.7632316]\n",
      "[-0.00777728]\n",
      "[0.7591069]\n",
      "[0.31946972]\n",
      "[0.29282668]\n",
      "[-0.8537436]\n",
      "[-0.9596646]\n",
      "[0.41237718]\n",
      "[0.6939846]\n",
      "[-0.09735057]\n",
      "[0.38238978]\n",
      "[-0.42753252]\n",
      "[0.12126191]\n",
      "[-0.9461265]\n",
      "[-0.16348988]\n",
      "[-0.8554307]\n",
      "[-0.12746157]\n",
      "[0.719787]\n",
      "[-0.29427242]\n",
      "[0.7488736]\n",
      "[-0.61143756]\n",
      "[0.7951999]\n",
      "[-0.55946726]\n",
      "[-0.588055]\n",
      "[0.3039604]\n",
      "[0.39933425]\n",
      "[0.66879946]\n",
      "[-0.20381826]\n",
      "[0.45714685]\n",
      "[0.10555106]\n",
      "[0.8717963]\n",
      "[0.588773]\n",
      "[-0.47131473]\n",
      "[-0.6912023]\n",
      "[-0.69439036]\n",
      "[0.6252475]\n",
      "[-0.48312837]\n",
      "[-0.00212554]\n",
      "[-0.09580802]\n",
      "[-0.71323997]\n",
      "[-0.47565964]\n",
      "[-0.64264274]\n",
      "[-0.6718551]\n",
      "[-0.78316295]\n",
      "[-0.68077433]\n",
      "[-0.7250015]\n",
      "[-0.79081726]\n",
      "[-0.5223485]\n",
      "[-0.8123135]\n",
      "[0.5344904]\n",
      "[0.46939766]\n",
      "[0.30076292]\n",
      "[-0.6288738]\n",
      "[-0.9837607]\n",
      "[-0.42648652]\n",
      "[-0.9193776]\n",
      "[-0.88927114]\n",
      "[0.8951959]\n",
      "[0.616838]\n",
      "[0.8688234]\n",
      "[0.36648905]\n",
      "[0.98799324]\n",
      "[0.8853063]\n",
      "[0.941036]\n",
      "[0.35484746]\n",
      "[-0.59113455]\n",
      "[-0.7842716]\n",
      "[0.6172621]\n",
      "[0.95598125]\n",
      "[-0.97210425]\n",
      "[0.20987862]\n",
      "[0.48624656]\n",
      "[-0.00940314]\n",
      "[0.7259757]\n",
      "[0.7202365]\n",
      "[-0.41003406]\n",
      "[-0.2620637]\n",
      "[0.49830723]\n",
      "[0.07977114]\n",
      "[-0.62536424]\n",
      "[0.08135049]\n",
      "[0.09956559]\n",
      "[0.02063761]\n",
      "[-0.8796645]\n",
      "[-0.8222136]\n",
      "[0.31149977]\n",
      "[-0.9483494]\n",
      "[-0.1556947]\n",
      "[-0.10732602]\n",
      "[0.7991131]\n",
      "[-0.38029656]\n",
      "[0.8551108]\n",
      "[0.27734113]\n",
      "[0.58030015]\n",
      "[-0.7257784]\n",
      "[0.31879124]\n",
      "[-0.30048123]\n",
      "[-0.9208584]\n",
      "[0.27929384]\n",
      "[-0.69216835]\n",
      "[0.9323713]\n",
      "[0.7392109]\n",
      "[0.07957803]\n",
      "[-0.23889592]\n",
      "[-0.20724778]\n",
      "[-0.65772223]\n",
      "[0.90316087]\n",
      "[0.31205124]\n",
      "[0.73240894]\n",
      "[-0.68583316]\n",
      "[0.86901695]\n",
      "[-0.15010755]\n",
      "[0.5043643]\n",
      "[-0.3382022]\n",
      "[-0.35087535]\n",
      "[-0.7647869]\n",
      "[0.01985469]\n",
      "[-0.8809867]\n",
      "[-0.68509674]\n",
      "[-0.7776435]\n",
      "[0.1285524]\n",
      "[0.8410667]\n",
      "[0.6445679]\n",
      "[-0.9583262]\n",
      "[-0.8028715]\n",
      "[-0.6605314]\n",
      "[-0.37574333]\n",
      "[-0.01168716]\n",
      "[-0.09409812]\n",
      "[0.13536124]\n",
      "[0.0224501]\n",
      "[0.36829147]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-6e41d9531a61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-234-f536e237cc56>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, steps_per_epoch, epochs, start_steps, max_ep_len)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# carry out update for each step experienced (episode length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexplore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# log progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-233-13ab274961fb>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iterations, batch_size)\u001b[0m\n\u001b[1;32m    107\u001b[0m             target_q_values = torch.min(\n\u001b[1;32m    108\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_soft_q_net1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_next_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_soft_q_net2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_next_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             ) - alpha * new_log_pi\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl_dev/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-199-bcc284af35a8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl_dev/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl_dev/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl_dev/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(int(1e6))\n",
    "\n",
    "env = NormalizedActions(gym.make(\"Pendulum-v0\"))\n",
    "# env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "agent = SAC(env, replay_buffer)\n",
    "\n",
    "train(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
